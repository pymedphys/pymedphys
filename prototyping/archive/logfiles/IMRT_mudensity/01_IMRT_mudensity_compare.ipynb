{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pymedphys.level1.msqconnect import mosaiq_connect\n",
    "from pymedphys.level1.configutilities import (\n",
    "    get_cache_filepaths, get_mu_density_parameters,\n",
    "    get_index, get_centre, get_sql_servers, get_sql_servers_list,\n",
    "    get_filepath\n",
    ")\n",
    "from pymedphys.level2.msqdelivery import multi_fetch_and_verify_mosaiq\n",
    "from pymedphys.level3.analyselogfiles import (\n",
    "    analyse_single_hash, find_consecutive_logfiles, get_field_id_key_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = get_index(config)\n",
    "file_hashes = np.array(list(index.keys()))\n",
    "\n",
    "field_types = np.array([\n",
    "    index[file_hash]['delivery_details']['field_type']\n",
    "    for file_hash in file_hashes\n",
    "])\n",
    "\n",
    "file_hashes = file_hashes[field_types == 'DMLC']\n",
    "\n",
    "is_qa = np.array([\n",
    "    index[file_hash]['delivery_details']['qa_mode']\n",
    "    for file_hash in file_hashes\n",
    "])\n",
    "\n",
    "file_hashes = file_hashes[np.invert(is_qa)]\n",
    "\n",
    "machine = np.array([\n",
    "    index[file_hash]['logfile_header']['machine']\n",
    "    for file_hash in file_hashes\n",
    "])\n",
    "\n",
    "# limit to RCCC for now\n",
    "file_hashes = file_hashes[(machine == '2619') | (machine == '2694')]\n",
    "np.random.shuffle(file_hashes)\n",
    "\n",
    "# might need course information\n",
    "index[file_hashes[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_grouped_fields = dict()\n",
    "field_id_grouped_hashes = dict()\n",
    "\n",
    "for file_hash in file_hashes:\n",
    "    delivery_details = index[file_hash]['delivery_details']\n",
    "    patient_id = delivery_details['patient_id']\n",
    "    field_id = delivery_details['field_id']\n",
    "    \n",
    "    if patient_id not in patient_grouped_fields:\n",
    "        patient_grouped_fields[patient_id] = set()\n",
    "        \n",
    "    patient_grouped_fields[patient_id].add(field_id)\n",
    "    \n",
    "    if field_id not in field_id_grouped_hashes:\n",
    "        field_id_grouped_hashes[field_id] = []\n",
    "        \n",
    "    field_id_grouped_hashes[field_id].append(file_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = list(patient_grouped_fields.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = '002911'\n",
    "fields = patient_grouped_fields[patient_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in fields:\n",
    "    print(field_id_grouped_hashes[field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mosaiq_connect('msqsql') as cursor:\n",
    "    for field in fields:\n",
    "        mosaiq_delivery_data = multi_fetch_and_verify_mosaiq(\n",
    "            cursor, field)\n",
    "        print(set(mosaiq_delivery_data.gantry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# field_id_key_map = get_field_id_key_map(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys.append(keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_consecutive_logfiles(file_hashes, index):\n",
    "    times = np.array([\n",
    "        index[key]['local_time']\n",
    "        for key in file_hashes\n",
    "    ]).astype(np.datetime64)\n",
    "\n",
    "    sort_reference = np.argsort(times)\n",
    "    file_hashes = file_hashes[sort_reference]\n",
    "    times = times[sort_reference]\n",
    "\n",
    "    hours_4 = np.array(60 * 60 * 4).astype(np.timedelta64)\n",
    "    split_locations = np.where(np.diff(times) >= hours_4)[0] + 1\n",
    "\n",
    "    return np.split(file_hashes, split_locations)\n",
    "\n",
    "\n",
    "field_id = list(fields)[0]\n",
    "keys = np.array(field_id_grouped_hashes[field_id])\n",
    "keys = np.hstack([keys, np.array(keys[1])])  # Spin this out into a unit test\n",
    "\n",
    "group_consecutive_logfiles(keys, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# field_id = list(fields)[0]\n",
    "# keys = np.array(field_id_grouped_hashes[field_id])\n",
    "# keys = np.hstack([keys, np.array(keys[1])])\n",
    "\n",
    "# times = np.array([\n",
    "#     index[key]['local_time']\n",
    "#     for key in keys\n",
    "# ]).astype(np.datetime64)\n",
    "\n",
    "# sort_reference = np.argsort(times)\n",
    "# keys = keys[sort_reference]\n",
    "# times = times[sort_reference]\n",
    "\n",
    "# hours_4 = np.array(60 * 60 * 4).astype(np.timedelta64)\n",
    "# split_locations = np.where(np.diff(times) >= hours_4)[0] + 1\n",
    "\n",
    "# np.split(times, split_locations)\n",
    "\n",
    "# # split_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.split(keys, split_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_hash = field_id_grouped_hashes[field_id][0]\n",
    "\n",
    "find_consecutive_logfiles(field_id_grouped_hashes, field_id, file_hash, index, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient_grouped_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# field_id_grouped_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_filepath(index, config, file_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient_grouped_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient_ids = [\n",
    "#     index[file_hash]['delivery_details']['patient_id']\n",
    "#     for file_hash in file_hashes\n",
    "# ]\n",
    "\n",
    "# # patient_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths = np.array([\n",
    "#     os.path.join(\n",
    "#         config['linac_logfile_data_directory'],\n",
    "#         'indexed',\n",
    "#         index[file_hash]['filepath'])\n",
    "#     for file_hash in file_hashes\n",
    "# ])\n",
    "# filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_hash = file_hashes[0]\n",
    "\n",
    "logfile_index_record = index[file_hash]\n",
    "field_id = logfile_index_record['delivery_details']['field_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mosaiq_connect('msqsql') as cursor:\n",
    "    mosaiq_delivery_data = multi_fetch_and_verify_mosaiq(\n",
    "        cursor, field_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaiq_delivery_data.gantry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
