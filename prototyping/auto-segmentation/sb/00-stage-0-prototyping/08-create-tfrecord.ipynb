{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import pathlib\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shapely.geometry\n",
    "import skimage.draw\n",
    "import skimage.filters\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pydicom\n",
    "\n",
    "import pymedphys\n",
    "import pymedphys._dicom.structure as dcm_struct\n",
    "\n",
    "from names import names_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all of the DICOM data within a directory called 'dicom' in here:\n",
    "data_path_root = pathlib.Path.home().joinpath('.data/dicom-ct-and-structures')\n",
    "dcm_paths = list(data_path_root.rglob('dicom/**/*.dcm'))\n",
    "\n",
    "# This will be the location of the numpy cache\n",
    "npz_directory = data_path_root.joinpath('npz_cache')\n",
    "npz_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# This will be the location of the tfrecord cache\n",
    "tfrecord_directory = data_path_root.joinpath('tfrecord_cache')\n",
    "tfrecord_directory.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the location of the DICOM header UID cache\n",
    "uid_cache_path = data_path_root.joinpath(\"uid-cache.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_surface_dice(reference, evaluation):\n",
    "    edge_reference = skimage.filters.scharr(reference)\n",
    "    edge_evaluation = skimage.filters.scharr(evaluation)\n",
    "    \n",
    "    score = (\n",
    "        np.sum(np.abs(edge_evaluation - edge_reference)) /\n",
    "        np.sum(edge_evaluation + edge_reference)\n",
    "    )\n",
    "    \n",
    "    return 1 - score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uid_cache(relative_paths):\n",
    "    relative_paths = [\n",
    "        str(path) for path in relative_paths\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        with open(uid_cache_path) as f:\n",
    "            uid_cache = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        uid_cache = {\n",
    "            \"ct_image_paths\": {},\n",
    "            \"structure_set_paths\": {},\n",
    "            \"ct_uid_to_structure_uid\": {},\n",
    "            \"paths_when_run\": []\n",
    "        }\n",
    "    \n",
    "    if set(uid_cache[\"paths_when_run\"]) == set(relative_paths):\n",
    "        return uid_cache\n",
    "    \n",
    "    dcm_headers = []\n",
    "    for dcm_path in dcm_paths:\n",
    "        dcm_headers.append(pydicom.read_file(\n",
    "            dcm_path, force=True, \n",
    "            specific_tags=['SOPInstanceUID', 'SOPClassUID', 'StudyInstanceUID']))\n",
    "        \n",
    "    ct_image_paths = {\n",
    "        str(header.SOPInstanceUID): str(path)\n",
    "        for header, path in zip(dcm_headers, relative_paths)\n",
    "        if header.SOPClassUID.name == \"CT Image Storage\"\n",
    "    }\n",
    "    \n",
    "    structure_set_paths = {\n",
    "        str(header.SOPInstanceUID): str(path)\n",
    "        for header, path in zip(dcm_headers, relative_paths)\n",
    "        if header.SOPClassUID.name == \"RT Structure Set Storage\"\n",
    "    }\n",
    "    \n",
    "    ct_uid_to_study_instance_uid = {\n",
    "        str(header.SOPInstanceUID): str(header.StudyInstanceUID)\n",
    "        for header in dcm_headers\n",
    "        if header.SOPClassUID.name == \"CT Image Storage\"\n",
    "    }\n",
    "    \n",
    "    study_instance_uid_to_structure_uid = {\n",
    "        str(header.StudyInstanceUID): str(header.SOPInstanceUID)\n",
    "        for header in dcm_headers\n",
    "        if header.SOPClassUID.name == \"RT Structure Set Storage\"\n",
    "    }\n",
    "    \n",
    "    ct_uid_to_structure_uid = {\n",
    "        ct_uid: study_instance_uid_to_structure_uid[study_uid]\n",
    "        for ct_uid, study_uid in ct_uid_to_study_instance_uid.items()\n",
    "    }\n",
    "    \n",
    "    uid_cache[\"ct_image_paths\"] = ct_image_paths\n",
    "    uid_cache[\"structure_set_paths\"] = structure_set_paths\n",
    "    uid_cache[\"ct_uid_to_structure_uid\"] = ct_uid_to_structure_uid    \n",
    "    uid_cache[\"paths_when_run\"] = relative_paths\n",
    "    \n",
    "    with open(uid_cache_path, \"w\") as f:\n",
    "        json.dump(uid_cache, f)\n",
    "        \n",
    "    return uid_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_paths = [\n",
    "    path.relative_to(data_path_root)\n",
    "    for path in dcm_paths\n",
    "]\n",
    "\n",
    "uid_cache = get_uid_cache(relative_paths)\n",
    "ct_image_paths = uid_cache[\"ct_image_paths\"]\n",
    "structure_set_paths = uid_cache[\"structure_set_paths\"]\n",
    "ct_uid_to_structure_uid = uid_cache[\"ct_uid_to_structure_uid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures_to_learn = list(set([item for key, item in names_map.items()]).difference({None}))\n",
    "structures_to_learn = sorted(structures_to_learn)\n",
    "structures_to_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_transformation_parameters(dcm_ct):\n",
    "    # From Matthew Coopers work in ../old/data_generator.py\n",
    "    \n",
    "    position = dcm_ct.ImagePositionPatient\n",
    "    spacing = [x for x in dcm_ct.PixelSpacing] + [dcm_ct.SliceThickness]\n",
    "    orientation = dcm_ct.ImageOrientationPatient\n",
    "\n",
    "    dx, dy, *_ = spacing\n",
    "    Cx, Cy, *_ = position\n",
    "    Ox, Oy = orientation[0], orientation[4]\n",
    "    \n",
    "    return dx, dy, Cx, Cy, Ox, Oy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_expanded_mask(expanded_mask, img_size, expansion):\n",
    "    return np.mean(np.mean(\n",
    "        tf.reshape(expanded_mask, (img_size, expansion, img_size, expansion)),\n",
    "        axis=1), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aliased_mask(contours, dcm_ct, expansion=5):\n",
    "    dx, dy, Cx, Cy, Ox, Oy = get_image_transformation_parameters(dcm_ct)\n",
    "    \n",
    "    ct_size = np.shape(dcm_ct.pixel_array)\n",
    "    x_grid = np.arange(Cx, Cx + ct_size[0]*dx*Ox, dx*Ox)\n",
    "    y_grid = np.arange(Cy, Cy + ct_size[1]*dy*Oy, dy*Oy)\n",
    "    \n",
    "    new_ct_size = np.array(ct_size) * expansion\n",
    "    \n",
    "    expanded_mask = np.zeros(new_ct_size)\n",
    "    \n",
    "    for xyz in contours:\n",
    "        x = np.array(xyz[0::3])\n",
    "        y = np.array(xyz[1::3])\n",
    "        z = xyz[2::3]\n",
    "\n",
    "        assert len(set(z)) == 1\n",
    "\n",
    "        r = (((y - Cy) / dy * Oy)) * expansion + (expansion - 1) * 0.5\n",
    "        c = (((x - Cx) / dx * Ox)) * expansion + (expansion - 1) * 0.5\n",
    "\n",
    "        expanded_mask = np.logical_or(expanded_mask, skimage.draw.polygon2mask(new_ct_size, np.array(list(zip(r, c)))))\n",
    "        \n",
    "    mask = reduce_expanded_mask(expanded_mask, ct_size[0], expansion)\n",
    "    mask = 2 * mask - 1\n",
    "    \n",
    "    return x_grid, y_grid, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contours_from_mask(x_grid, y_grid, mask):\n",
    "    cs = plt.contour(x_grid, y_grid, mask, [0]);\n",
    "    \n",
    "    contours = [\n",
    "        path.vertices for path in cs.collections[0].get_paths()\n",
    "    ]\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numpy_input_output(ct_uid):\n",
    "    structure_uid = ct_uid_to_structure_uid[ct_uid]\n",
    "\n",
    "    structure_set_path = data_path_root.joinpath(structure_set_paths[structure_uid])\n",
    "\n",
    "    structure_set = pydicom.read_file(\n",
    "        structure_set_path, \n",
    "        force=True, \n",
    "        specific_tags=['ROIContourSequence', 'StructureSetROISequence'])\n",
    "    \n",
    "    number_to_name_map = {\n",
    "        roi_sequence_item.ROINumber: names_map[roi_sequence_item.ROIName]\n",
    "        for roi_sequence_item in structure_set.StructureSetROISequence\n",
    "        if names_map[roi_sequence_item.ROIName] is not None\n",
    "    }\n",
    "    \n",
    "    contours_by_ct_uid = {}\n",
    "\n",
    "    for roi_contour_sequence_item in structure_set.ROIContourSequence:\n",
    "        try:\n",
    "            structure_name = number_to_name_map[roi_contour_sequence_item.ReferencedROINumber]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        for contour_sequence_item in roi_contour_sequence_item.ContourSequence:\n",
    "            ct_uid = contour_sequence_item.ContourImageSequence[0].ReferencedSOPInstanceUID\n",
    "\n",
    "            try:\n",
    "                _ = contours_by_ct_uid[ct_uid]\n",
    "            except KeyError:\n",
    "                contours_by_ct_uid[ct_uid] = dict()\n",
    "\n",
    "            try:\n",
    "                contours_by_ct_uid[ct_uid][structure_name].append(contour_sequence_item.ContourData)\n",
    "            except KeyError:\n",
    "                contours_by_ct_uid[ct_uid][structure_name] = [contour_sequence_item.ContourData]\n",
    "                \n",
    "    ct_path = data_path_root.joinpath(ct_image_paths[ct_uid])\n",
    "    dcm_ct = pydicom.read_file(ct_path, force=True)\n",
    "    dcm_ct.file_meta.TransferSyntaxUID = pydicom.uid.ImplicitVRLittleEndian\n",
    "\n",
    "    ct_size = np.shape(dcm_ct.pixel_array)\n",
    "    \n",
    "    contours_on_this_slice = contours_by_ct_uid[ct_uid].keys()\n",
    "\n",
    "    masks = np.nan * np.ones((*ct_size, len(structures_to_learn)))\n",
    "\n",
    "    for i, structure in enumerate(structures_to_learn):\n",
    "        if not structure in contours_on_this_slice:\n",
    "            masks[:,:,i] = np.zeros(ct_size) - 1\n",
    "\n",
    "            continue\n",
    "\n",
    "        original_contours = contours_by_ct_uid[ct_uid][structure]\n",
    "        x_grid, y_grid, masks[:,:,i] = calculate_aliased_mask(original_contours, dcm_ct)\n",
    "        \n",
    "    np.shape(masks)\n",
    "    assert np.sum(np.isnan(masks)) == 0\n",
    "    \n",
    "    return dcm_ct.pixel_array, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_input_output_from_cache(ct_uid, structures_to_learn):\n",
    "    npz_path = npz_directory.joinpath(f'{ct_uid}.npz')\n",
    "    structures_to_learn_cache_path = npz_directory.joinpath('structures_to_learn.json')\n",
    "    \n",
    "    try:\n",
    "        with open(structures_to_learn_cache_path) as f:\n",
    "            structures_to_learn_cache = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        structures_to_learn_cache = []\n",
    "        \n",
    "    if structures_to_learn_cache != structures_to_learn:\n",
    "        logging.warning(\"Structures to learn has changed. Dumping npz cache.\")\n",
    "        for path in npz_directory.glob('*.npz'):\n",
    "            path.unlink()\n",
    "        with open(structures_to_learn_cache_path, \"w\") as f:\n",
    "            json.dump(structures_to_learn, f)\n",
    "\n",
    "    try:\n",
    "        data = np.load(npz_path)\n",
    "        input_array = data['input_array']\n",
    "        output_array = data['output_array']\n",
    "    except FileNotFoundError:\n",
    "        input_array, output_array = create_numpy_input_output(ct_uid)\n",
    "        np.savez(npz_path, input_array=input_array, output_array=output_array)\n",
    "        \n",
    "    return input_array, output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ct_uids = list(ct_image_paths.keys())\n",
    "random.shuffle(all_ct_uids)\n",
    "\n",
    "def from_numpy_generator():\n",
    "    for ct_uid in all_ct_uids:\n",
    "        input_array, output_array = numpy_input_output_from_cache(\n",
    "            ct_uid, structures_to_learn)\n",
    "        input_array = input_array[:,:,None]\n",
    "        \n",
    "        yield ct_uid, input_array, output_array\n",
    "        \n",
    "from_numpy_generator_params = (\n",
    "    (tf.string, tf.int32, tf.float64), \n",
    "    (tf.TensorShape(()), tf.TensorShape([512, 512, 1]), tf.TensorShape([512, 512, 41]))\n",
    ")\n",
    "\n",
    "from_numpy_dataset = tf.data.Dataset.from_generator(\n",
    "    from_numpy_generator, *from_numpy_generator_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def serialise(ct_uid, input_array, output_array):\n",
    "    ct_uid = tf.io.serialize_tensor(ct_uid)\n",
    "    input_array = tf.io.serialize_tensor(input_array)\n",
    "    output_array = tf.io.serialize_tensor(output_array)\n",
    "    \n",
    "    feature = {\n",
    "        'ct_uid': _bytes_feature(ct_uid),\n",
    "        'input_array': _bytes_feature(input_array),\n",
    "        'output_array': _bytes_feature(output_array),\n",
    "    }\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "\n",
    "for ct_uid, input_array, output_array in from_numpy_dataset.take(1):\n",
    "    serialise(ct_uid, input_array, output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details on this from https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "def tf_serialise(ct_uid, input_array, output_array):\n",
    "    tf_string = tf.py_function(\n",
    "        serialise,\n",
    "        (ct_uid, input_array, output_array),\n",
    "        tf.string\n",
    "    )\n",
    "    return tf.reshape(tf_string, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialised_dataset = from_numpy_dataset.map(tf_serialise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialised_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_path = str(tfrecord_directory.joinpath('test.tfrecord'))\n",
    "writer = tf.data.experimental.TFRecordWriter(tfrecord_path)\n",
    "writer.write(serialised_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymedphys-master",
   "language": "python",
   "name": "pymedphys-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
