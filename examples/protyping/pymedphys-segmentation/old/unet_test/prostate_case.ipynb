{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import (absolute_import, division,\n",
    "                        print_function, unicode_literals)\n",
    "from builtins import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import sys\n",
    " print(sys.executable)\n",
    " print(sys.version)\n",
    " print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "\n",
    "# Data extraction\n",
    "#DATA_PATH = \"/home/jupyter/prostate_ct_small\"\n",
    "DATA_PATH = \"/home/matthew/priv/PROSTATE_TEST/\"\n",
    "STRUCTURE_NAMES = [\"BLADDER\"]\n",
    "\n",
    "# Train/Valid/Test\n",
    "SPLIT_RATIO = (0.7, 0.2, 0.1)\n",
    "\n",
    "# DATA SHAPE\n",
    "CONTEXT = 1\n",
    "BATCH_SIZE = 1\n",
    "SIZE = 128\n",
    "INPUT_SHAPE = (2*CONTEXT + 1, SIZE, SIZE, 1)\n",
    "OUTPUT_SHAPE = (1, SIZE, SIZE, len(STRUCTURE_NAMES))\n",
    "\n",
    "\n",
    "# MODEL COMPILING\n",
    "EPOCHS = 1\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS = tf.nn.sigmoid_cross_entropy_with_logits\n",
    "METRICS = ['accuracy']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "def get_paths(data_path, context):\n",
    "    patient_paths = glob.glob(data_path + \"/*\")\n",
    "\n",
    "    context_paths = glob.glob(data_path + \"/*/*CT*\", recursive=True)\n",
    "    context_paths.sort()\n",
    "    input_paths = [\n",
    "            glob.glob(path + \"/*CT*\")[context:-context] for path in patient_paths\n",
    "        ]\n",
    "    input_paths = [item for sublist in input_paths for item in sublist]\n",
    "    random.shuffle(input_paths)\n",
    "\n",
    "    label_paths = glob.glob(data_path + \"/*/*RS*\", recursive=True)\n",
    "\n",
    "    #assert len(context_paths) - (len(label_paths) * 2 *\n",
    "    #                             context) == len(input_paths)\n",
    "\n",
    "    return input_paths, context_paths, label_paths\n",
    "\n",
    "\n",
    "def split_paths(input_paths, ratio):\n",
    "    num = len(input_paths)\n",
    "    num_train = int(num * ratio[0] // 1)\n",
    "    num_valid = int(num * ratio[1] // 1)\n",
    "    num_test = int(num * ratio[2] // 1)\n",
    "\n",
    "    #print(f\"Total: {num} = Train: {num_train} + Valid: {num_valid} + Test: {num_test}\")\n",
    "    print(\"Total:\", num)\n",
    "    print(\"==========\")\n",
    "    print(\"Train:\", num_train)\n",
    "    print(\"Valid:\", num_valid)\n",
    "    print(\"Test:\", num_test)\n",
    "\n",
    "    train_paths = input_paths[0:num_train]\n",
    "    valid_paths = input_paths[num_train:num_train + num_valid]\n",
    "    test_paths = input_paths[num_train + num_valid:]\n",
    "    return train_paths, valid_paths, test_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, valid_paths, test_paths = split_paths(input_paths, SPLIT_RATIO)\n",
    "\n",
    "print(\"----------\")\n",
    "print(\"Batch size\", BATCH_SIZE)\n",
    "print(\"Steps per epoch (# batches per epoch)\", len(train_paths) // BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load generator\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "import skimage.draw\n",
    "import numpy as np\n",
    "# Perhaps use tf.io instead\n",
    "import pydicom\n",
    "from pathlib import Path\n",
    "import skimage.transform\n",
    "\n",
    "# TODO - Modulate get_item\n",
    "\n",
    "class DataGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, input_paths, context_paths, label_paths, context,\n",
    "                 batch_size, structure_names, resize):\n",
    "        self.input_paths = input_paths\n",
    "        self.context_paths = context_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.context = context\n",
    "        self.batch_size = batch_size\n",
    "        self.structure_names = structure_names\n",
    "        self.resize = resize\n",
    "\n",
    "        for path in self.label_paths:\n",
    "            _ = self.pre_cached_structures(path)\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    @functools.lru_cache()\n",
    "    def pre_cached_structures(self, path):\n",
    "        return pydicom.dcmread(path, force=True)\n",
    "\n",
    "    def get_parent_dir(self, path):\n",
    "        return Path(path).parent.name\n",
    "\n",
    "    # TODO\n",
    "    # def resize_vol(volume, shape):\n",
    "    #     for s in volume:\n",
    "    #         skimage.transform.resize(s, shape)\n",
    "    #     return volume\n",
    "    \n",
    "    # TODO\n",
    "    # def normal:\n",
    "\n",
    "\n",
    "    def __getitem__(self, batch_index):\n",
    "\n",
    "        if (batch_index + 1) * self.batch_size > len(self.input_paths):\n",
    "            self.batch_size = len(\n",
    "                self.input_paths) - batch_index * self.batch_size\n",
    "\n",
    "        batch_paths = self.input_paths[batch_index *\n",
    "                                       self.batch_size:(batch_index + 1) *\n",
    "                                       self.batch_size]\n",
    "\n",
    "        batch_inputs = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for image_path in batch_paths:\n",
    "            # Get parent dir\n",
    "            parent_dir = self.get_parent_dir(image_path)\n",
    "            # Get mask path\n",
    "            mask_path = [s for s in self.label_paths if parent_dir in s][0]\n",
    "            # Get index\n",
    "            image_index = self.context_paths.index(image_path)\n",
    "            # Get context\n",
    "            input_paths = self.context_paths[image_index -\n",
    "                                             self.context:image_index +\n",
    "                                             self.context + 1]\n",
    "\n",
    "            try:\n",
    "                assert len(input_paths) == 2 * self.context + 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            ###################### IMAGE LOOP ###################################\n",
    "\n",
    "            images = []\n",
    "            for dcm_path in input_paths:\n",
    "                dicom_ct = pydicom.dcmread(dcm_path, force=True)\n",
    "                try:\n",
    "                    dicom_ct.file_meta.TransferSyntaxUID\n",
    "                except AttributeError:\n",
    "                    dicom_ct.file_meta.TransferSyntaxUID = (\n",
    "                        pydicom.uid.ImplicitVRLittleEndian)\n",
    "                image = dicom_ct.pixel_array\n",
    "                image = skimage.transform.resize(image, (self.resize, self.resize))\n",
    "                images = images + [image]\n",
    "\n",
    "            batch_inputs.append(images)\n",
    "\n",
    "            ####################### MASK LOOP ####################################\n",
    "\n",
    "            img = pydicom.dcmread(image_path, force=True)\n",
    "            img_position = img.ImagePositionPatient\n",
    "            img_spacing = [x for x in img.PixelSpacing] + [img.SliceThickness]\n",
    "            img_orientation = img.ImageOrientationPatient\n",
    "\n",
    "            dicom_structures = self.pre_cached_structures(mask_path)\n",
    "\n",
    "            assert img.FrameOfReferenceUID == dicom_structures.StructureSetROISequence[\n",
    "                0].ReferencedFrameOfReferenceUID\n",
    "\n",
    "            dcm_rs_struct_names = [\n",
    "                structure.ROIName\n",
    "                for structure in dicom_structures.StructureSetROISequence\n",
    "            ]\n",
    "\n",
    "            structure_names = self.structure_names\n",
    "\n",
    "            names_to_pull = [\n",
    "                name for name in dcm_rs_struct_names if name in structure_names\n",
    "            ]\n",
    "            try:\n",
    "                assert len(names_to_pull) == len(structure_names)\n",
    "            except:\n",
    "                batch_inputs.pop()\n",
    "                continue\n",
    "\n",
    "            structure_indexes = [\n",
    "                dcm_rs_struct_names.index(name) for name in names_to_pull\n",
    "            ]\n",
    "\n",
    "            mask = np.zeros(shape=(1, 512, 512, len(structure_indexes)))\n",
    "\n",
    "            dx, dy, *rest = img_spacing\n",
    "            Cx, Cy, *rest = img_position\n",
    "            Ox, Oy = img_orientation[0], img_orientation[4]\n",
    "\n",
    "            for mask_index, structure_index in enumerate(structure_indexes):\n",
    "                z = [\n",
    "                    z_slice.ContourData[2::3][0]\n",
    "                    for z_slice in dicom_structures.\n",
    "                    ROIContourSequence[structure_index].ContourSequence\n",
    "                ]\n",
    "\n",
    "                try:\n",
    "                    indexes = z.index(img_position[2])\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    len(indexes)\n",
    "                except:\n",
    "                    indexes = [indexes]\n",
    "\n",
    "                for index in indexes:\n",
    "                    xyz = dicom_structures.ROIContourSequence[\n",
    "                        structure_index].ContourSequence[index].ContourData\n",
    "\n",
    "                    x = np.array(xyz[0::3])\n",
    "                    y = np.array(xyz[1::3])\n",
    "\n",
    "                    r = (y - Cy) / dy * Oy\n",
    "                    c = (x - Cx) / dx * Ox\n",
    "\n",
    "                    rr, cc = skimage.draw.polygon(r, c)\n",
    "\n",
    "                    mask[:, rr, cc, mask_index] = True\n",
    "            \n",
    "            mask = skimage.transform.resize(mask, (1, self.resize, self.resize, len(structure_indexes)))\n",
    "                                            \n",
    "            batch_labels.append(mask)\n",
    "\n",
    "        ###################### RETURNS ###################################\n",
    "        batch_inputs = np.array(batch_inputs)\n",
    "        batch_inputs = batch_inputs[..., np.newaxis]\n",
    "\n",
    "        #batch_input = np.array(batch_inputs)\n",
    "        batch_labels = np.array(batch_labels)\n",
    "\n",
    "        return batch_inputs, batch_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of batches per epoch\n",
    "        return int(np.ceil(len(self.input_paths) / float(self.batch_size)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGen(train_paths,\n",
    "                       context_paths,\n",
    "                       label_paths,\n",
    "                       context=CONTEXT,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       structure_names=STRUCTURE_NAMES,resize=SIZE)\n",
    "\n",
    "valid_gen = DataGen(valid_paths,\n",
    "                    context_paths,\n",
    "                    label_paths,\n",
    "                    context=CONTEXT,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    structure_names=STRUCTURE_NAMES, resize=SIZE)\n",
    "\n",
    "test_gen = DataGen(test_paths,\n",
    "                    context_paths,\n",
    "                    label_paths,\n",
    "                    context=CONTEXT,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    structure_names=STRUCTURE_NAMES, resize=SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random batch\n",
    "\n",
    "from random import randint\n",
    "batch_index = randint(0, round(len(test_paths) / BATCH_SIZE)-1)\n",
    "print(\"Batch index:\", batch_index, \"/\", round(len(test_paths) / BATCH_SIZE)-1)\n",
    "test_inputs, test_labels = test_gen.__getitem__(batch_index=batch_index)\n",
    "\n",
    "print(\"inputs:\", test_inputs.shape)\n",
    "print(\"labels:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an random example input and label from batch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "index = randint(0, test_inputs.shape[0]-1)\n",
    "print(\"index:\", index, \"/\", test_inputs.shape[0]-1)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "axes[0].imshow(test_inputs[index,0,:,:,0])\n",
    "axes[1].imshow(test_labels[index,0,:,:,0])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL THE ABOVE SHOULD BE WORKING!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(input_shape, output_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Conv3D(1, (3,3,3), padding='same')(inputs)\n",
    "    x = tf.keras.layers.AveragePooling3D(pool_size=(11, 1, 1),\n",
    "                                              strides=1,\n",
    "                                              padding='valid')(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv3D(1, (3,3,3), padding='same')(x)\n",
    "    x = tf.keras.layers.AveragePooling3D(pool_size=(11, 1, 1),\n",
    "                                              strides=1,\n",
    "                                              padding='valid')(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv3D(output_shape[-1], 1, activation=None)(x)\n",
    "    x = tf.keras.activations.sigmoid(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SHAPE\n",
    "CONTEXT = 10\n",
    "BATCH_SIZE = 50\n",
    "SIZE = 128\n",
    "\n",
    "INPUT_SHAPE = (2*CONTEXT + 1, SIZE, SIZE, 1)\n",
    "OUTPUT_SHAPE = (1, SIZE, SIZE, len(STRUCTURE_NAMES))\n",
    "\n",
    "train_gen = DataGen(train_paths,\n",
    "                       context_paths,\n",
    "                       label_paths,\n",
    "                       context=CONTEXT,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       structure_names=STRUCTURE_NAMES,resize=SIZE)\n",
    "\n",
    "valid_gen = DataGen(valid_paths,\n",
    "                    context_paths,\n",
    "                    label_paths,\n",
    "                    context=CONTEXT,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    structure_names=STRUCTURE_NAMES, resize=SIZE)\n",
    "\n",
    "test_gen = DataGen(test_paths,\n",
    "                    context_paths,\n",
    "                    label_paths,\n",
    "                    context=CONTEXT,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    structure_names=STRUCTURE_NAMES, resize=SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_model(INPUT_SHAPE, OUTPUT_SHAPE)\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=train_gen,\n",
    "                    validation_data=valid_gen,\n",
    "                    steps_per_epoch=len(train_paths) // BATCH_SIZE,\n",
    "                    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random testing batch for predictions\n",
    "\n",
    "from random import randint\n",
    "batch_index = randint(0, round(len(test_paths) / BATCH_SIZE)-1)\n",
    "print(\"Batch index:\", batch_index, \"/\", round(len(test_paths) / BATCH_SIZE)-1)\n",
    "test_inputs, test_labels = test_gen.__getitem__(batch_index=batch_index)\n",
    "\n",
    "print(\"inputs:\", test_inputs.shape)\n",
    "print(\"labels:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(test_inputs)\n",
    "predicts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an random example input and label from batch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "index = randint(0, test_inputs.shape[0]-1)\n",
    "print(\"index:\", index, \"/\", test_inputs.shape[0]-1)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 5), sharex=True, sharey=True)\n",
    "axes[0].imshow(test_inputs[index,0,:,:,0])\n",
    "axes[1].imshow(test_labels[index,0,:,:,0])\n",
    "axes[2].imshow(predicts[index,0,:,:,0])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET FUNCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def down_block(x, m, n, c, size):\n",
    "    Zcrop = int((n / 2) * (size - 1))\n",
    "    crop = tf.keras.layers.Cropping3D(cropping=(Zcrop, 0, 0))(x)\n",
    "    crop = tf.keras.layers.Conv3D(c, 1, activation=None)(crop)\n",
    "\n",
    "    result = tf.keras.layers.ReLU()(x)\n",
    "    result = x\n",
    "    for repeat in range(m):\n",
    "        result = tf.keras.layers.Conv3D(c, (1, 3, 3),\n",
    "                                        strides=1,\n",
    "                                        padding='same')(result)\n",
    "        result = tf.keras.layers.ReLU()(result)\n",
    "\n",
    "    for repeat in range(n):\n",
    "        result = tf.keras.layers.Conv3D(c, (1, 3, 3),\n",
    "                                        strides=1,\n",
    "                                        padding='same', activation=None)(result)\n",
    "        result = tf.keras.layers.Conv3D(c, (size, 1, 1),\n",
    "                                        strides=1,\n",
    "                                        padding='valid', activation=None)(result)\n",
    "        \n",
    "        if repeat != range(n)[-1]:\n",
    "            result = tf.keras.layers.ReLU()(result)\n",
    "\n",
    "    result = tf.keras.layers.Add()([crop, result])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def pool(x, size_xy):\n",
    "    result = tf.keras.layers.AveragePooling3D(pool_size=(1, size_xy, size_xy),\n",
    "                                              strides=None,\n",
    "                                              padding='valid')(x)\n",
    "    return result\n",
    "\n",
    "\n",
    "def fc_block(x, r, inp = 1024, out=256):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = tf.keras.layers.Conv3D(inp, (1, 8, 8),\n",
    "                                    strides=1,\n",
    "                                    padding='valid')(x)\n",
    "    for repeat in range(r):\n",
    "        crop = result\n",
    "        # TODO: Should this be a dense layer with RelU activation instead?\n",
    "        result = tf.keras.layers.ReLU()(result)\n",
    "        result = tf.keras.layers.Add()([crop, result])\n",
    "\n",
    "    result = tf.keras.layers.ReLU()(result)\n",
    "    result = tf.keras.layers.Reshape((1, 8, 8, out))(x)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def up_block(x, m, c):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    crop = tf.keras.layers.Conv3D(c, 1, activation=None)(x)\n",
    "\n",
    "    result = tf.keras.layers.ReLU()(x)\n",
    "    for repeat in range(m):\n",
    "        result = tf.keras.layers.Conv3D(c, (1, 3, 3),\n",
    "                                        strides=1,\n",
    "                                        padding='same')(result)\n",
    "        result = tf.keras.layers.ReLU()(result)\n",
    "    result = tf.keras.layers.Add()([crop, result])\n",
    "    return result\n",
    "\n",
    "\n",
    "def upscale(x, size_xy):\n",
    "    result = tf.keras.layers.UpSampling3D(size=(1, size_xy, size_xy))(x)\n",
    "    return result\n",
    "\n",
    "\n",
    "def stack(x, skip):\n",
    "    # NOTE axis 0 is the batch\n",
    "    # axis 1 is along z = depth dimension\n",
    "    result = tf.keras.layers.Concatenate(axis=1)([x, skip])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=INPUT_SHAPE)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.dtypes.cast(inputs, tf.float16)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 layer UNET 512 * 512 * 10 context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_3_512(input_shape, output_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype=tf.float16)\n",
    "    #inputs = tf.dtypes.cast(x, tf.float16)\n",
    "    print(inputs)\n",
    "    skips = []\n",
    "\n",
    "    x = down_block(inputs, 0, 1, 64, 5)\n",
    "    print(x)\n",
    "    skips.append(x)\n",
    "    x = pool(x, 4)\n",
    "    print(x)\n",
    "\n",
    "    x = down_block(x, 0, 2, 128, 5)\n",
    "    print(x)\n",
    "    skips.append(x)\n",
    "    x = pool(x, 4)\n",
    "    print(x)\n",
    "\n",
    "    x = down_block(x, 0, 2, 256, 5)\n",
    "    print(x)\n",
    "    skips.append(x)\n",
    "\n",
    "    x = fc_block(x, 2)\n",
    "    print(x)\n",
    "\n",
    "    x = stack(skips[-1], x)\n",
    "    print(x)\n",
    "    x = up_block(x, 1, 128)\n",
    "    print(x)\n",
    "\n",
    "    x = upscale(x, 4)\n",
    "    print(x)\n",
    "    x = stack(skips[-2], x)\n",
    "    print(x)\n",
    "    x = up_block(x, 1, 64)\n",
    "    print(x)\n",
    "\n",
    "    x = upscale(x, 4)\n",
    "    print(x)\n",
    "    x = stack(skips[-3], x)\n",
    "    print(x)\n",
    "    x = up_block(x, 1, 1)\n",
    "    print(x)\n",
    "\n",
    "    # TODO Shouldnt this be 1 * 1 * 1 conv\n",
    "    x = tf.keras.layers.Conv3D(filters=output_shape[-1],\n",
    "                               kernel_size=(28, 1, 1),\n",
    "                               strides=1,\n",
    "                               activation='sigmoid',\n",
    "                               padding='valid')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SHAPE\n",
    "CONTEXT = 10\n",
    "BATCH_SIZE = 2\n",
    "SIZE = 512\n",
    "\n",
    "INPUT_SHAPE = (2*CONTEXT + 1, SIZE, SIZE, 1)\n",
    "OUTPUT_SHAPE = (1, SIZE, SIZE, len(STRUCTURE_NAMES))\n",
    "\n",
    "train_gen = DataGen(train_paths,\n",
    "                       context_paths,\n",
    "                       label_paths,\n",
    "                       context=CONTEXT,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       structure_names=STRUCTURE_NAMES,resize=SIZE)\n",
    "\n",
    "valid_gen = DataGen(valid_paths,\n",
    "                    context_paths,\n",
    "                    label_paths,\n",
    "                    context=CONTEXT,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    structure_names=STRUCTURE_NAMES, resize=SIZE)\n",
    "\n",
    "test_gen = DataGen(test_paths,\n",
    "                    context_paths,\n",
    "                    label_paths,\n",
    "                    context=CONTEXT,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    structure_names=STRUCTURE_NAMES, resize=SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_3_512(INPUT_SHAPE, OUTPUT_SHAPE)\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit_generator(generator=train_gen,\n",
    "#                     validation_data=valid_gen,\n",
    "#                     steps_per_epoch=len(train_paths) // BATCH_SIZE,\n",
    "#                     epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Layer unet 128 * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_3_128(input_shape, output_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    skips = []\n",
    "\n",
    "    x = down_block(inputs, 0, 1, 64, 1)\n",
    "    print(x)\n",
    "    skips.append(x)\n",
    "    x = pool(x, 4)\n",
    "    print(x)\n",
    "\n",
    "    x = down_block(x, 0, 1, 128, 1)\n",
    "    print(x)\n",
    "    skips.append(x)\n",
    "    x = pool(x, 4)\n",
    "    print(x)\n",
    "\n",
    "    x = down_block(x, 0, 1, 256, 1)\n",
    "    print(x)\n",
    "    skips.append(x)\n",
    "\n",
    "    x = fc_block(x, 2)\n",
    "    print(\"FC\", x)\n",
    "\n",
    "\n",
    "    x = stack(skips[-1], x)\n",
    "    print(x)\n",
    "    x = up_block(x, 1, 128)\n",
    "    print(x)\n",
    "\n",
    "    x = upscale(x, 4)\n",
    "    print(x)\n",
    "    x = stack(skips[-2], x)\n",
    "    print(x)\n",
    "    x = up_block(x, 1, 64)\n",
    "    print(x)\n",
    "\n",
    "    x = upscale(x, 4)\n",
    "    print(x)\n",
    "    x = stack(skips[-3], x)\n",
    "    print(x)\n",
    "    x = up_block(x, 1, 1)\n",
    "    print(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv3D(filters=output_shape[-1],\n",
    "                               kernel_size=(64, 1, 1),\n",
    "                               strides=1,\n",
    "                               activation='sigmoid',\n",
    "                               padding='valid')(x)\n",
    "    print(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SHAPE\n",
    "CONTEXT = 10\n",
    "BATCH_SIZE = 2\n",
    "SIZE = 128\n",
    "\n",
    "INPUT_SHAPE = (2*CONTEXT + 1, SIZE, SIZE, 1)\n",
    "OUTPUT_SHAPE = (1, SIZE, SIZE, len(STRUCTURE_NAMES))\n",
    "\n",
    "train_gen = DataGen(train_paths,\n",
    "                       context_paths,\n",
    "                       label_paths,\n",
    "                       context=CONTEXT,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       structure_names=STRUCTURE_NAMES,resize=SIZE)\n",
    "\n",
    "valid_gen = DataGen(valid_paths,\n",
    "                    context_paths,\n",
    "                    label_paths,\n",
    "                    context=CONTEXT,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    structure_names=STRUCTURE_NAMES, resize=SIZE)\n",
    "\n",
    "test_gen = DataGen(test_paths,\n",
    "                    context_paths,\n",
    "                    label_paths,\n",
    "                    context=CONTEXT,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    structure_names=STRUCTURE_NAMES, resize=SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_3_128(INPUT_SHAPE, OUTPUT_SHAPE)\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=train_gen,\n",
    "                    validation_data=valid_gen,\n",
    "                    steps_per_epoch=len(train_paths) // BATCH_SIZE,\n",
    "                    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Layer unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model_2_layer(input_shape, output_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    skips = []\n",
    "\n",
    "    x = down_block(inputs, 0, 1, 128, 9)\n",
    "    print(x)\n",
    "    skips.append(x)\n",
    "    x = pool(x, 8)\n",
    "    print(x)\n",
    "\n",
    "    x = down_block(x, 0, 2, 256, 7)\n",
    "    print(x)\n",
    "    skips.append(x)\n",
    "    x = pool(x, 8)\n",
    "    print(x)\n",
    "\n",
    "\n",
    "    x = fc_block(x, 2)\n",
    "    print(x)\n",
    "\n",
    "\n",
    "    x = upscale(x, 8)\n",
    "    print(x)\n",
    "    x = stack(skips[-1], x)\n",
    "    print(x)\n",
    "    x = up_block(x, 1, 128)\n",
    "    print(x)\n",
    "\n",
    "    x = upscale(x, 8)\n",
    "    print(x)\n",
    "    x = stack(skips[-2], x)\n",
    "    print(x)\n",
    "    x = up_block(x, 1, 1)\n",
    "    print(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv3D(filters=output_shape[-1],\n",
    "                               kernel_size=(15, 1, 1),\n",
    "                               strides=1,\n",
    "                               activation='sigmoid',\n",
    "                               padding='valid')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SHAPE\n",
    "CONTEXT = 10\n",
    "BATCH_SIZE = 2\n",
    "SIZE = 512\n",
    "INPUT_SHAPE = (2*CONTEXT + 1, SIZE, SIZE, 1)\n",
    "\n",
    "train_gen = DataGen(train_paths,\n",
    "                       context_paths,\n",
    "                       label_paths,\n",
    "                       context=CONTEXT,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       structure_names=STRUCTURE_NAMES,resize=SIZE)\n",
    "\n",
    "valid_gen = DataGen(valid_paths,\n",
    "                    context_paths,\n",
    "                    label_paths,\n",
    "                    context=CONTEXT,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    structure_names=STRUCTURE_NAMES, resize=SIZE)\n",
    "\n",
    "test_gen = DataGen(test_paths,\n",
    "                    context_paths,\n",
    "                    label_paths,\n",
    "                    context=CONTEXT,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    structure_names=STRUCTURE_NAMES, resize=SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_3_128(INPUT_SHAPE, OUTPUT_SHAPE)\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=train_gen,\n",
    "                    validation_data=valid_gen,\n",
    "                    steps_per_epoch=len(train_paths) // BATCH_SIZE,\n",
    "                    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Layer unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model_1_layer(input_shape, output_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    skips = []\n",
    "\n",
    "    x = down_block(inputs, 0, 3, 32, 3)\n",
    "    print(x)\n",
    "    skips.append(x)\n",
    "    x = pool(x, 8)\n",
    "    print(x)\n",
    "\n",
    "\n",
    "    x = fc_block(x, 2, inp = 2048, out=32)\n",
    "    print(\"fc\", x)\n",
    "\n",
    "    x = upscale(x, 8)\n",
    "    print(x)\n",
    "    x = stack(skips[-1], x)\n",
    "    print(x)\n",
    "    x = up_block(x, 1, 1)\n",
    "    print(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv3D(filters=output_shape[-1],\n",
    "                               kernel_size=(30, 1, 1),\n",
    "                               strides=1,\n",
    "                               activation='sigmoid',\n",
    "                               padding='valid')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SHAPE\n",
    "CONTEXT = 10\n",
    "BATCH_SIZE = 50\n",
    "SIZE = 512\n",
    "\n",
    "INPUT_SHAPE = (2*CONTEXT + 1, SIZE, SIZE, 1)\n",
    "OUTPUT_SHAPE = (1, SIZE, SIZE, len(STRUCTURE_NAMES))\n",
    "\n",
    "train_gen = DataGen(train_paths,\n",
    "                       context_paths,\n",
    "                       label_paths,\n",
    "                       context=CONTEXT,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       structure_names=STRUCTURE_NAMES,resize=SIZE)\n",
    "\n",
    "valid_gen = DataGen(valid_paths,\n",
    "                    context_paths,\n",
    "                    label_paths,\n",
    "                    context=CONTEXT,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    structure_names=STRUCTURE_NAMES, resize=SIZE)\n",
    "\n",
    "test_gen = DataGen(test_paths,\n",
    "                    context_paths,\n",
    "                    label_paths,\n",
    "                    context=CONTEXT,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    structure_names=STRUCTURE_NAMES, resize=SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_3_128(INPUT_SHAPE, OUTPUT_SHAPE)\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=train_gen,\n",
    "                    validation_data=valid_gen,\n",
    "                    steps_per_epoch=len(train_paths) // BATCH_SIZE,\n",
    "                    epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
