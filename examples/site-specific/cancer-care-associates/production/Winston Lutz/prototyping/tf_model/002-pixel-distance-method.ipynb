{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import IPython\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes it so any changes in pymedphys is automatically\n",
    "# propagated into the notebook without needing a kernel reset.\n",
    "from IPython.lib.deepreload import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reversed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "parameter_size = 2\n",
    "\n",
    "\n",
    "def convolve_block(filters, kernel, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters, kernel, strides=1, padding='same',\n",
    "            kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def evacuation_hatch(x):\n",
    "    evacuation_stages = [\n",
    "        tf.keras.layers.GlobalMaxPooling2D(),\n",
    "        tf.keras.layers.Dense(parameter_size),\n",
    "    ]\n",
    "    for evac in evacuation_stages:\n",
    "        x = evac(x)\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "def Model():\n",
    "    down_blocks = [\n",
    "        convolve_block(64, 3, apply_batchnorm=False),\n",
    "        convolve_block(128, 3),\n",
    "    ]\n",
    "    fully_connected_blocks = [\n",
    "        tf.keras.layers.Dense(128),\n",
    "        tf.keras.layers.Dense(128),\n",
    "        tf.keras.layers.Dense(128),\n",
    "    ]\n",
    "    up_blocks = [\n",
    "        convolve_block(128, 3),\n",
    "        convolve_block(64, 3),\n",
    "    ]\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=[None,None,1], batch_size=None)\n",
    "    x = inputs\n",
    "    \n",
    "    skips = []\n",
    "#     evacs = []\n",
    "    \n",
    "    for down in down_blocks:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "#         evacs.append(evacuation_hatch(x))\n",
    "\n",
    "    skips = reversed(skips)\n",
    "    \n",
    "    for fc in fully_connected_blocks:\n",
    "        init = x\n",
    "        x = fc(x)\n",
    "        x = tf.keras.layers.Add()([x, init])   \n",
    "#         evacs.append(evacuation_hatch(x))\n",
    "        \n",
    "    for up, skip in zip(up_blocks, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Add()([x, skip])\n",
    "#         evacs.append(evacuation_hatch(x))\n",
    "    \n",
    "    x = evacuation_hatch(x)\n",
    "#     x = tf.keras.layers.Concatenate()(evacs)\n",
    "    x = tf.keras.layers.Dense(parameter_size)(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymedphys._mocks import wlutz, profiles\n",
    "from pymedphys._wlutz import reporting, interppoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = interppoints.translate_and_rotate_transform([1,1], 20)\n",
    "interppoints.apply_transform(2, 3, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_dataset(grid_size, pixels_per_mm):\n",
    "    bounding_pixel_centre_val_mm = (grid_size / 2 - 0.5) / pixels_per_mm   \n",
    "\n",
    "    field_centre = np.random.uniform(-50, 50, size=2)\n",
    "    field_side_lengths = np.exp(np.random.uniform(np.log(10), np.log(150), size=2))\n",
    "    field_penumbra = np.random.uniform(1, 5)\n",
    "\n",
    "    field_rotation = np.random.uniform(-180, 180)\n",
    "\n",
    "    transform = interppoints.translate_and_rotate_transform(field_centre, field_rotation)\n",
    "\n",
    "    bb_centre_before_transform = [\n",
    "        np.random.uniform(\n",
    "            -field_side_lengths[0]/2, field_side_lengths[0]/2),\n",
    "        np.random.uniform(\n",
    "            -field_side_lengths[1]/2, field_side_lengths[1]/2)\n",
    "    ]\n",
    "    bb_centre = interppoints.apply_transform(*bb_centre_before_transform, transform)\n",
    "\n",
    "    bb_diameter = tf.random.uniform((), 0.5, 10).numpy()\n",
    "    bb_max_attenuation = tf.random.uniform((), 0.1, 0.5).numpy()\n",
    "\n",
    "\n",
    "    field = profiles.create_rectangular_field_function(\n",
    "        field_centre, field_side_lengths, field_penumbra, field_rotation\n",
    "    )\n",
    "    bb_penumbra = field_penumbra / 3\n",
    "    bb_attenuation_map = wlutz.create_bb_attenuation_func(\n",
    "        bb_diameter, bb_penumbra, bb_max_attenuation\n",
    "    )\n",
    "\n",
    "    x = np.linspace(\n",
    "        -bounding_pixel_centre_val_mm,\n",
    "        bounding_pixel_centre_val_mm,\n",
    "        grid_size\n",
    "    )\n",
    "    xx, yy = np.meshgrid(x, x)\n",
    "\n",
    "    without_bb = field(xx, yy)\n",
    "\n",
    "    def field_with_bb(x, y):\n",
    "        return field(x, y) * bb_attenuation_map(x - bb_centre[0], y - bb_centre[1])\n",
    "\n",
    "    with_bb = field_with_bb(xx, yy)\n",
    "        \n",
    "            \n",
    "    parameters = tf.concat([field_centre, field_side_lengths, [field_rotation], bb_centre, [bb_diameter]], 0)\n",
    "    img = tf.convert_to_tensor(with_bb, dtype=tf.float32)\n",
    "            \n",
    "    return parameters, img, xx, yy\n",
    "\n",
    "parameters, img, xx, yy = create_single_dataset(1024, 4)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.pcolormesh(xx, yy, img)\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_dataset(batch_size, min_size=32, max_size=128):\n",
    "    \n",
    "    min_size = tf.convert_to_tensor(min_size, dtype=tf.int32)\n",
    "    max_size = tf.convert_to_tensor(max_size, dtype=tf.int32)\n",
    "    \n",
    "    def dataset_generator():\n",
    "        image_size_for_current_batch = tf.random.uniform((), min_size, max_size, dtype=tf.int32).numpy()\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            yield create_single_dataset(image_size_for_current_batch)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        dataset_generator, \n",
    "        (tf.float32, tf.float32), \n",
    "        (tf.TensorShape([8]), tf.TensorShape([None, None]))\n",
    "    )\n",
    "\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameters, img in create_pipeline_dataset(2).take(3):    \n",
    "    dim = img.shape\n",
    "    print(dim)\n",
    "    for i in range(dim[0]):        \n",
    "        plt.figure()\n",
    "        plt.pcolormesh(img[i, :, :])\n",
    "        plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameters(parameters):\n",
    "    parameters = {\n",
    "        'field_centre': (parameters[0], parameters[1]),\n",
    "        'field_side_lengths': (parameters[2], parameters[3]),\n",
    "        'field_rotation': parameters[4] * 180,\n",
    "        'bb_centre': (parameters[5], parameters[6]),\n",
    "        'bb_diameter': parameters[7]\n",
    "    }\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def extract_parameters_from_both(predicted_parameters, image_parameters):\n",
    "    \n",
    "    output_image_parameters = extract_parameters(image_parameters)\n",
    "    \n",
    "    output_predicted_parameters = {\n",
    "        'field_centre': (image_parameters[0], image_parameters[1]),\n",
    "        'field_side_lengths': (image_parameters[2], image_parameters[3]),\n",
    "        'field_rotation': image_parameters[4] * 180,\n",
    "        'bb_centre': (predicted_parameters[0], predicted_parameters[1]),\n",
    "        'bb_diameter': image_parameters[7]\n",
    "    }\n",
    "    \n",
    "    return output_predicted_parameters, output_image_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_figure(image, field_centre, field_side_lengths, field_rotation, bb_centre, bb_diameter):\n",
    "    dim = image.shape\n",
    "    x = np.linspace(-1, 1, dim[0])\n",
    "    y = x\n",
    "    \n",
    "    return reporting.image_analysis_figure(\n",
    "        x, y, np.array(image),\n",
    "        np.array(bb_centre), np.array(field_centre), np.array(field_rotation),\n",
    "        bb_diameter, field_side_lengths, penumbra=0.2, units=''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_figures(model, batch_images, batch_ground_truth_parameters):\n",
    "    batch_dim = batch_images.shape\n",
    "    num_batches = batch_dim[0]\n",
    "    image_shape = dim[1]\n",
    "    x = np.linspace(-1, 1, image_shape)\n",
    "    y = x\n",
    "    \n",
    "    batch_predicted_parameters = model(batch_images, training=True)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        \n",
    "        predicted_parameters, ground_truth_parameters = extract_parameters_from_both(\n",
    "            batch_predicted_parameters[i, :],\n",
    "            batch_ground_truth_parameters[i, :]\n",
    "        )\n",
    "    \n",
    "        fig, axs = create_figure(batch_images[i,:,:], **ground_truth_parameters)\n",
    "        axs[0,0].set_title(\"Ground Truth\")\n",
    "\n",
    "        fig, axs = create_figure(batch_images[i,:,:], **predicted_parameters)\n",
    "        axs[0,0].set_title(\"Predicted\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameters, img in create_pipeline_dataset(1).take(1):    \n",
    "    dim = img.shape\n",
    "    print(dim)\n",
    "    for i in range(dim[0]):        \n",
    "        plt.figure()\n",
    "        plt.pcolormesh(img[i, :, :])\n",
    "        plt.axis('equal')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    results_figures(model, img, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_parameters(parameters):\n",
    "    new_parameters = tf.concat([\n",
    "        parameters[:, 0:2],\n",
    "        parameters[:, 3:4],\n",
    "        parameters[:, 2:3],\n",
    "        parameters[:, 4:5] + 0.5,\n",
    "        parameters[:, 5::]\n",
    "    ], axis=-1)\n",
    "    \n",
    "    return new_parameters\n",
    "\n",
    "\n",
    "for parameters, img in create_pipeline_dataset(1).take(1):\n",
    "    flipped_parameters = flip_parameters(parameters)\n",
    "    \n",
    "    print(parameters)\n",
    "    print(flipped_parameters)\n",
    "    \n",
    "    batch_dim = img.shape\n",
    "    num_batches = batch_dim[0]\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        fig, axs = create_figure(img[i,:,:], **extract_parameters(parameters[i,:]))\n",
    "        axs[0,0].set_title(\"Original\")\n",
    "\n",
    "        fig, axs = create_figure(img[i,:,:], **extract_parameters(flipped_parameters[i,:]))\n",
    "        axs[0,0].set_title(\"Flipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_rotation_diff(predicted_parameters, ground_truth_parameters):\n",
    "    predicted_rotations = predicted_parameters[:, 4] * 180\n",
    "    ground_truth_rotations = ground_truth_parameters[:, 4] * 180\n",
    "    \n",
    "    diff_rotation = (predicted_rotations - ground_truth_rotations) % 180\n",
    "    diff_rotation = tf.reduce_min(tf.stack([diff_rotation, 180 - diff_rotation]), axis=0)\n",
    "    \n",
    "    return diff_rotation\n",
    "\n",
    "\n",
    "def adjust_rotation(parameters, degrees):\n",
    "    new_parameters = tf.concat([\n",
    "        parameters[:,0:4],\n",
    "        parameters[:,4:5] + degrees/180,\n",
    "        parameters[:,5::]\n",
    "    ], axis=-1)\n",
    "    \n",
    "    return new_parameters\n",
    "\n",
    "\n",
    "for parameters, img in create_pipeline_dataset(4).take(1):    \n",
    "    print(determine_rotation_diff(parameters, parameters))\n",
    "    print(determine_rotation_diff(parameters, adjust_rotation(parameters, 1)))\n",
    "    print(determine_rotation_diff(parameters, adjust_rotation(parameters, 90)))\n",
    "    print(determine_rotation_diff(parameters, adjust_rotation(parameters, 180)))\n",
    "    print(determine_rotation_diff(parameters, adjust_rotation(parameters, 181)))\n",
    "    print(determine_rotation_diff(parameters, adjust_rotation(parameters, 179)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_diff_with_rotation(predicted_parameters, ground_truth_parameters):\n",
    "    diff = tf.concat([\n",
    "        predicted_parameters[:, 0:4] - ground_truth_parameters[:, 0:4],\n",
    "        determine_rotation_diff(predicted_parameters, ground_truth_parameters)[:, None] / 180,\n",
    "        predicted_parameters[:, 5::] - ground_truth_parameters[:, 5::],\n",
    "    ], axis=-1)\n",
    "    \n",
    "    return diff\n",
    "\n",
    "\n",
    "for parameters, img in create_pipeline_dataset(4).take(1):\n",
    "    \n",
    "    flipped_parameters = flip_parameters(parameters)\n",
    "    print(determine_diff_with_rotation(parameters, parameters))\n",
    "    print(determine_diff_with_rotation(parameters, flipped_parameters))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print(determine_diff_with_rotation(parameters, parameters + 1))\n",
    "    print(determine_diff_with_rotation(parameters, adjust_rotation(parameters, 1)))\n",
    "    print(determine_diff_with_rotation(parameters, adjust_rotation(parameters, 90)))\n",
    "    print(determine_diff_with_rotation(parameters, adjust_rotation(parameters, 180)))\n",
    "    print(determine_diff_with_rotation(parameters, adjust_rotation(parameters, 181)))\n",
    "    print(determine_diff_with_rotation(parameters, adjust_rotation(parameters, 179)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_corrected_diff(predicted_parameters, ground_truth_parameters):\n",
    "    flipped_predicted_parameters = flip_parameters(predicted_parameters)\n",
    "    \n",
    "    non_flipped_diff = determine_diff_with_rotation(predicted_parameters, ground_truth_parameters)\n",
    "    flipped_diff = determine_diff_with_rotation(flipped_predicted_parameters, ground_truth_parameters)\n",
    "    \n",
    "    non_flipped_sum_sqrd = tf.reduce_sum(\n",
    "        non_flipped_diff**2, axis=-1\n",
    "    )\n",
    "    flipped_sum_sqrd = tf.reduce_sum(\n",
    "        flipped_diff**2, axis=-1\n",
    "    )\n",
    "    \n",
    "    minimisation_stack = tf.stack([\n",
    "        non_flipped_sum_sqrd, flipped_sum_sqrd\n",
    "    ])\n",
    "#     print(minimisation_stack)\n",
    "    \n",
    "    min_index = tf.argmin(minimisation_stack, axis=0)\n",
    "    a_range = tf.range(min_index.shape[0], dtype=tf.int64)\n",
    "    min_index = tf.stack([\n",
    "        min_index, a_range\n",
    "    ], axis=-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(min_index)\n",
    "    \n",
    "#     print(non_flipped_diff)\n",
    "#     print(flipped_diff)\n",
    "    \n",
    "    both_diffs = tf.stack([\n",
    "        non_flipped_diff, flipped_diff\n",
    "    ])\n",
    "#     print(both_diffs)\n",
    "#     print(min_index)\n",
    "\n",
    "    diff = tf.gather_nd(\n",
    "        both_diffs, min_index\n",
    "    )\n",
    "    \n",
    "    return diff\n",
    "\n",
    "\n",
    "for parameters, img in create_pipeline_dataset(2).take(1):\n",
    "    flipped_parameters = flip_parameters(parameters)\n",
    "    print(determine_corrected_diff(parameters, parameters + 0.5))\n",
    "#     print(determine_corrected_diff(parameters, flipped_parameters))\n",
    "#     print(determine_corrected_diff(flipped_parameters, parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def just_bb_loss(predicted_parameters, ground_truth_parameters):\n",
    "    return tf.sqrt(tf.reduce_sum((predicted_parameters - ground_truth_parameters[:, 6:8])**2, axis=-1))\n",
    "    \n",
    "\n",
    "for parameters, img in create_pipeline_dataset(5).take(2):\n",
    "    print(just_bb_loss(parameters[:, 6:8] + 1, parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(predicted_parameters, ground_truth_parameters):\n",
    "    diff = determine_corrected_diff(predicted_parameters, ground_truth_parameters)\n",
    "    loss = tf.abs(diff)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# for parameters, img in create_pipeline_dataset(1).take(1):\n",
    "#     print(parameters)\n",
    "#     print(loss_function(parameters, parameters))\n",
    "#     print(loss_function(parameters, parameters + 1))\n",
    "    \n",
    "    \n",
    "for parameters, img in create_pipeline_dataset(5).take(1):\n",
    "    print(loss_function(parameters, parameters + 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "fixed_vals = {\n",
    "#     'field_centre': [0, 0],\n",
    "#     'field_side_lengths': [1, 1],\n",
    "#     'field_rotation': 0,\n",
    "# #     'bb_centre': [0, 0],\n",
    "#     'bb_diameter': 0.4,\n",
    "# #     'image_size': 32,\n",
    "}\n",
    "\n",
    "test_dataset = create_pipeline_dataset(1)\n",
    "train_dataset = create_pipeline_dataset(BATCH_SIZE)\n",
    "\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def train_step(ground_truth_parameters, input_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted_parameters = model(input_image, training=True)\n",
    "#         loss = loss_function(predicted_parameters, ground_truth_parameters)\n",
    "        loss = just_bb_loss(predicted_parameters, ground_truth_parameters)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    reduced_loss = tf.reduce_mean(loss, axis=0)\n",
    "    return reduced_loss\n",
    "        \n",
    "        \n",
    "concrete_train_step = train_step.get_concrete_function(\n",
    "    tf.TensorSpec(shape=[BATCH_SIZE, 8], dtype=tf.float32), \n",
    "    tf.TensorSpec(shape=[BATCH_SIZE, None, None], dtype=tf.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_parameters(tensor_parameters):\n",
    "#     parameters = {\n",
    "#         'field_centre': (tensor_parameters[0], tensor_parameters[1]),\n",
    "#         'field_side_lengths': (tensor_parameters[2], tensor_parameters[3]),\n",
    "#         'field_rotation': tensor_parameters[4] * 180,\n",
    "#         'bb_centre': (tensor_parameters[5], tensor_parameters[6]),\n",
    "#         'bb_diameter': tensor_parameters[7]\n",
    "#     }\n",
    "    \n",
    "#     return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_ds, epochs, test_ds):\n",
    "    logging_index = tf.convert_to_tensor(0, dtype=tf.int64)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for parameters, img in test_ds.take(1):\n",
    "            results_figures(model, img, parameters)\n",
    "\n",
    "        iters_per_epoch = 10\n",
    "        for parameters, img in tqdm.tqdm(train_ds.take(iters_per_epoch), total=iters_per_epoch):\n",
    "            reduced_loss = concrete_train_step(parameters, img)\n",
    "            \n",
    "            logging_index += 1\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('bb_distance', reduced_loss, step=logging_index)\n",
    "            \n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                            time.time()-start))\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "poetry run tensorboard --logdir examples/site-specific/cancer-care-associates/production/Winston\\ Lutz/prototyping/tf_model/logs/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "\n",
    "log_dir=\"logs/\"\n",
    "summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "fit(train_dataset, EPOCHS, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymedphys-master",
   "language": "python",
   "name": "pymedphys-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
