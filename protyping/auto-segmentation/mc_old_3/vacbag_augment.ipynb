{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "path = \"./vet_dataset_cleaned/\"\n",
    "\n",
    "# The meaning of life\n",
    "random.seed(42)  \n",
    "np.random.seed(42)\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import paths as paths\n",
    "\n",
    "\n",
    "patient_paths = paths.get_patient_paths(path)\n",
    "patient_paths.sort()\n",
    "\n",
    "img_paths = [glob.glob(path + \"/img/*\") for path in patient_paths]\n",
    "mask_paths = [glob.glob(path + \"/mask/*\") for path in patient_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = int(len(img_paths) * 0.15 // 1)\n",
    "test = int(len(img_paths) * 0.1 // 1)\n",
    "train = int(len(img_paths) - valid - test)\n",
    "\n",
    "print(train, valid, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = paths.flatten_list(img_paths[0:train])\n",
    "train_truths = paths.flatten_list(mask_paths[0:train])\n",
    "\n",
    "train_inputs.sort()\n",
    "train_truths.sort()\n",
    "\n",
    "valid_inputs = paths.flatten_list(img_paths[train:train+valid])\n",
    "valid_truths = paths.flatten_list(mask_paths[train:train+valid])\n",
    "\n",
    "valid_inputs.sort()\n",
    "valid_truths.sort()\n",
    "\n",
    "test_inputs = paths.flatten_list(img_paths[train+valid:])\n",
    "test_truths = paths.flatten_list(mask_paths[train+valid:])\n",
    "\n",
    "test_inputs.sort()\n",
    "test_truths.sort()\n",
    "\n",
    "print(len(train_inputs))\n",
    "print(len(valid_inputs))\n",
    "print(len(test_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_together(inputs, truths):\n",
    "    shuffle_together = list(zip(inputs, truths))\n",
    "    random.shuffle(shuffle_together)\n",
    "    inputs, truths = zip(*shuffle_together)\n",
    "    return inputs, truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_truths = shuffle_together(train_inputs, train_truths)\n",
    "valid_inputs, valid_truths = shuffle_together(valid_inputs, valid_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "import cv2\n",
    "import skimage.transform\n",
    "\n",
    "\n",
    "def gaussian_noise(img, mean=0, sigma=0.003):\n",
    "    img = img.copy()\n",
    "    noise = np.random.normal(mean, sigma, img.shape)\n",
    "    mask_overflow_upper = img+noise >= 1.0\n",
    "    mask_overflow_lower = img+noise < 0\n",
    "    noise[mask_overflow_upper] = 1.0\n",
    "    noise[mask_overflow_lower] = 0\n",
    "    img = img + noise\n",
    "    return img\n",
    "\n",
    "def random_crop_resize(img, label, crop_size=492):\n",
    "    size_img = img.shape\n",
    "    size_label = label.shape\n",
    "    crop_size = random.randint(crop_size, img.shape[0]-1)\n",
    "    crop_size = (crop_size, crop_size)\n",
    "\n",
    "    # \"Crop size should be less than image size\"\n",
    "    assert crop_size[0] <= img.shape[0] and crop_size[1] <= img.shape[1]\n",
    "\n",
    "    w, h = img.shape[:2]\n",
    "    x, y = np.random.randint(h-crop_size[0]), np.random.randint(w-crop_size[1])\n",
    "\n",
    "    img = img[y:y+crop_size[0], x:x+crop_size[1],:]\n",
    "    img = skimage.transform.resize(img, size_img)\n",
    "\n",
    "    label = label[y:y+crop_size[0], x:x+crop_size[1],:]\n",
    "    label = skimage.transform.resize(label, size_label)\n",
    "    return img, label\n",
    "\n",
    "def affine_transform(image, label, alpha_affine=0.5, random_state=None):\n",
    "\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape\n",
    "    shape_size = shape[:2]\n",
    "    center_square = np.float32(shape_size) // 2\n",
    "    square_size = min(shape_size) // 3\n",
    "    pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n",
    "    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "\n",
    "    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
    "    image = image[...,np.newaxis]\n",
    "    label = cv2.warpAffine(label, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def data_augment(img, mask, chance=1):\n",
    "    # flip l/r\n",
    "    if random.uniform(0,1) < chance:\n",
    "        img = cv2.flip( img, 1 )\n",
    "        mask = cv2.flip( mask, 1 )\n",
    "        if len(img.shape) == 2:\n",
    "            img = img[...,np.newaxis]\n",
    "        if len(mask.shape) == 2:\n",
    "            mask = mask[...,np.newaxis]\n",
    "\n",
    "    # random crop and resize\n",
    "    if random.uniform(0,1) < chance:\n",
    "        img, mask = random_crop_resize(img, mask)\n",
    "        if len(mask.shape) == 2:\n",
    "            label = label[...,np.newaxis]\n",
    "\n",
    "    # random affine transformation\n",
    "    if random.uniform(0,1) < chance:\n",
    "        img, mask = affine_transform(img, mask, alpha_affine=20)\n",
    "        if len(img.shape) == 2:\n",
    "            img = img[...,np.newaxis]\n",
    "        if len(mask.shape) == 2:\n",
    "            mask = mask[...,np.newaxis]\n",
    "            \n",
    "    # random gaussian noise\n",
    "    if random.uniform(0,1) < chance:\n",
    "        img = gaussian_noise(img)\n",
    "        \n",
    "    return img, mask\n",
    "\n",
    "def normalise(x, mean, std):\n",
    "    return (x - mean) / std\n",
    "\n",
    "\n",
    "def read_array_list(arr_path_list):\n",
    "    return np.array([np.load(arr_path) for arr_path in arr_path_list])\n",
    "\n",
    "\n",
    "def data_generator(input_paths, truth_paths, batch_size, mean, std, augment=False):\n",
    "    batch_number = 0\n",
    "    while batch_number < len(input_paths):\n",
    "        if batch_number + batch_size <= len(input_paths):\n",
    "            batch_input_paths = input_paths[batch_number: batch_number + batch_size]\n",
    "            batch_truth_paths = truth_paths[batch_number: batch_number + batch_size]\n",
    "        else:\n",
    "            batch_input_paths = input_paths[batch_number:]\n",
    "            batch_truth_paths = truth_paths[batch_number:]\n",
    "        batch_number += batch_size\n",
    "        \n",
    "        batch_imgs = []\n",
    "        batch_masks = []\n",
    "        \n",
    "        for x, y in zip(batch_input_paths, batch_truth_paths):\n",
    "            x = np.load(x)\n",
    "            y = np.load(y)\n",
    "            if augment is True:\n",
    "               \n",
    "                \n",
    "                x = normalise(x, mean, std)\n",
    "                \n",
    "                if random.uniform(0,1) < 0.5:\n",
    "                    x, y = data_augment(x, y, chance=0.5)\n",
    "\n",
    "            batch_imgs.append(x)\n",
    "            batch_masks.append(y)\n",
    "        \n",
    "        yield np.array(batch_imgs, dtype=np.float32), np.array(batch_masks, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "import cv2\n",
    "import skimage.transform\n",
    "\n",
    "class make_gen(tf.keras.utils.Sequence):\n",
    "    def __init__( self, input_paths, label_paths, batch_size, training_mean, training_std, shuffle_on_end=True, augment=True):\n",
    "        self.input_paths = input_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.training_mean = training_mean\n",
    "        self.training_std = training_std\n",
    "        self.shuffle_on_end = shuffle_on_end\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of batches per epoch\n",
    "        return int(np.ceil(len(self.input_paths) / float(self.batch_size)))\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        if self.shuffle_on_end == True:\n",
    "            self.inputs, self.truths = self.suffle_together(self.inputs, self.truths)\n",
    "            \n",
    "    def shuffle_together(self, inputs, truths):\n",
    "        shuffle_together = list(zip(inputs, truths))\n",
    "        random.shuffle(shuffle_together)\n",
    "        inputs, truths = zip(*shuffle_together)\n",
    "        return inputs, truths\n",
    "    \n",
    "    \n",
    "    def gaussian_noise(self, img, mean=0, sigma=0.003):\n",
    "        img = img.copy()\n",
    "        noise = np.random.normal(mean, sigma, img.shape)\n",
    "        mask_overflow_upper = img+noise >= 1.0\n",
    "        mask_overflow_lower = img+noise < 0\n",
    "        noise[mask_overflow_upper] = 1.0\n",
    "        noise[mask_overflow_lower] = 0\n",
    "        img = img + noise\n",
    "        return img\n",
    "\n",
    "    def random_crop_resize(self, img, label, crop_size=492):\n",
    "        size_img = img.shape\n",
    "        size_label = label.shape\n",
    "        crop_size = random.randint(crop_size, img.shape[0]-1)\n",
    "        crop_size = (crop_size, crop_size)\n",
    "\n",
    "        # \"Crop size should be less than image size\"\n",
    "        assert crop_size[0] <= img.shape[0] and crop_size[1] <= img.shape[1]\n",
    "\n",
    "        w, h = img.shape[:2]\n",
    "        x, y = np.random.randint(h-crop_size[0]), np.random.randint(w-crop_size[1])\n",
    "\n",
    "        img = img[y:y+crop_size[0], x:x+crop_size[1],:]\n",
    "        img = skimage.transform.resize(img, size_img)\n",
    "\n",
    "        label = label[y:y+crop_size[0], x:x+crop_size[1],:]\n",
    "        label = skimage.transform.resize(label, size_label)\n",
    "        return img, label\n",
    "\n",
    "    def affine_transform(self, image, label, alpha_affine=0.5, random_state=None):\n",
    "\n",
    "        if random_state is None:\n",
    "            random_state = np.random.RandomState(None)\n",
    "\n",
    "        shape = image.shape\n",
    "        shape_size = shape[:2]\n",
    "        center_square = np.float32(shape_size) // 2\n",
    "        square_size = min(shape_size) // 3\n",
    "        pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n",
    "        pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n",
    "        M = cv2.getAffineTransform(pts1, pts2)\n",
    "\n",
    "        image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
    "        image = image[...,np.newaxis]\n",
    "        label = cv2.warpAffine(label, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "    def data_augment(self, img, mask, chance=1):\n",
    "        # flip l/r\n",
    "        if random.uniform(0,1) < chance:\n",
    "            img = cv2.flip( img, 1 )\n",
    "            mask = cv2.flip( mask, 1 )\n",
    "            if len(img.shape) == 2:\n",
    "                img = img[...,np.newaxis]\n",
    "            if len(mask.shape) == 2:\n",
    "                mask = mask[...,np.newaxis]\n",
    "\n",
    "        # random crop and resize\n",
    "        if random.uniform(0,1) < chance:\n",
    "            img, mask = self.random_crop_resize(img, mask)\n",
    "            if len(img.shape) == 2:\n",
    "                img = img[...,np.newaxis]\n",
    "            if len(mask.shape) == 2:\n",
    "                label = label[...,np.newaxis]\n",
    "\n",
    "        # random affine transformation\n",
    "        if random.uniform(0,1) < chance:\n",
    "            img, mask = self.affine_transform(img, mask, alpha_affine=20)\n",
    "            if len(img.shape) == 2:\n",
    "                img = img[...,np.newaxis]\n",
    "            if len(mask.shape) == 2:\n",
    "                mask = mask[...,np.newaxis]\n",
    "            \n",
    "        # random gaussian noise\n",
    "        if random.uniform(0,1) < chance:\n",
    "            img = self.gaussian_noise(img)\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "    def normalise(self, x, mean, std):\n",
    "        return (x - mean) / std\n",
    "\n",
    "\n",
    "    def read_array_list(self, arr_path_list):\n",
    "        return np.array([np.load(arr_path) for arr_path in arr_path_list])\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, batch_index):\n",
    "        # Get batch at index: batch_index\n",
    "\n",
    "        # Handle case when not enough inputs for a full batch\n",
    "        # NOTE this changes the values for the generator\n",
    "        # Need to initialise again if triggered\n",
    "        if (batch_index + 1) * self.batch_size > len(self.input_paths):\n",
    "            batch_size = len(self.input_paths) - batch_index * self.batch_size\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        # Extract input paths for one batch at index: batch_index\n",
    "        batch_input_paths = self.input_paths[ batch_index * batch_size : (batch_index + 1) * batch_size ] \n",
    "        batch_label_paths = self.label_paths[ batch_index * batch_size : (batch_index + 1) * batch_size ] \n",
    "        \n",
    "        batch_imgs = []\n",
    "        batch_masks = []\n",
    "        \n",
    "        for x, y in zip(batch_input_paths, batch_label_paths):\n",
    "            x = np.load(x)\n",
    "            y = np.load(y)\n",
    "            if self.augment is True:\n",
    "                x = self.normalise(x, self.training_mean, self.training_std)\n",
    "                \n",
    "                if random.uniform(0,1) < 0.5:\n",
    "                    x, y = self.data_augment(x, y, chance=0.5)\n",
    "\n",
    "            batch_imgs.append(x)\n",
    "            batch_masks.append(y)\n",
    "        \n",
    "        return np.array(batch_imgs, dtype=np.float32), np.array(batch_masks, dtype=np.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = 168.3172158554484\n",
    "data_std = 340.21625683608994\n",
    "batch_size = 3\n",
    "\n",
    "\n",
    "valid_gen = make_gen(valid_inputs,\n",
    "                     valid_truths,\n",
    "                     batch_size,\n",
    "                     data_mean,\n",
    "                     data_std, \n",
    "                     shuffle_on_end=False, \n",
    "                     augment=False)\n",
    "\n",
    "\n",
    "train_gen = make_gen(train_inputs,\n",
    "                     train_truths,\n",
    "                     batch_size,\n",
    "                     data_mean,\n",
    "                     data_std, \n",
    "                     shuffle_on_end=True, \n",
    "                     augment=True)\n",
    "\n",
    "\n",
    "test_gen = make_gen(test_inputs,\n",
    "                     test_truths,\n",
    "                     batch_size,\n",
    "                     data_mean,\n",
    "                     data_std, \n",
    "                     shuffle_on_end=False, \n",
    "                     augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_gen.__getitem__(0)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train as train\n",
    "import tensorflow as tf\n",
    "\n",
    "INITIAL_LR = 1e-4\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(lr = INITIAL_LR)\n",
    "LOSS = tf.keras.losses.BinaryCrossentropy()\n",
    "WEIGHTS = \"./vacbag/initial.hdf5\"\n",
    "\n",
    "\n",
    "\n",
    "model = train.compile_model(1, OPTIMIZER, LOSS, weights=WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = train_gen.__len__()\n",
    "valid_steps = valid_gen.__len__()\n",
    "\n",
    "print(steps_per_epoch)\n",
    "print(valid_steps)\n",
    "\n",
    "print(\"\\n Training...\")\n",
    "train_history = model.fit(train_gen,\n",
    "                          epochs=epochs,\n",
    "                          steps_per_epoch=steps_per_epoch,\n",
    "                          validation_steps=valid_steps,\n",
    "                          validation_data=valid_gen,\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"test_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss as loss\n",
    "\n",
    "INITIAL_LR = 1e-4\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(lr = INITIAL_LR)\n",
    "LOSS = loss.dsc_loss\n",
    "WEIGHTS = \"test_weights.hdf5\"\n",
    "\n",
    "\n",
    "\n",
    "model = train.compile_model(1, OPTIMIZER, LOSS, weights=WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " w\n",
    "    \n",
    "    \n",
    "    from datetime import datetime\n",
    "    \n",
    "    date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    file_name = model_save + \"ep_{epoch:02d}_vl{val_loss:.2f}_\" + str(date_time)\n",
    "    model_save = file_name + \".hdf5\"\n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=stop_patience, verbose=1, restore_best_weights=True)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(factor=lr_scale, patience=lr_patience, verbose=1)\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(model_save, save_weights_only=True, verbose=1)\n",
    "    csv_logger = tf.keras.callbacks.CSVLogger(file_name, separator=',', append=False)\n",
    "    tensor_board = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n",
    "                                                                 histogram_freq=5,\n",
    "                                                                 write_graph=True,\n",
    "                                                                 write_images=True,\n",
    "                                                                 embeddings_freq=5,\n",
    "                                                                 update_freq='epoch')\n",
    "    \n",
    "    callbacks = [early_stopping, model_checkpoint, reduce_lr, csv_logger, tensor_board]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "steps_per_epoch = train_gen.__len__()\n",
    "valid_steps = valid_gen.__len__()\n",
    "\n",
    "print(steps_per_epoch)\n",
    "print(valid_steps)\n",
    "\n",
    "print(\"\\n Training...\")\n",
    "train_history = model.fit(train_gen,\n",
    "                          epochs=epochs,\n",
    "                          steps_per_epoch=steps_per_epoch,\n",
    "                          validation_steps=valid_steps,\n",
    "                          validation_data=valid_gen,\n",
    "                          callbacks=callbacks,\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./vacbag/2_dice_after_bce_41_0.04_2020-04-29-16-21-27.hdf5_12_0.35_2020-04-29-18-54-51.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = test_gen.__getitem__(0)\n",
    "x.shape, y.shape\n",
    "\n",
    "plt.plot()\n",
    "plt.imshow(x[0,...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out[1,...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
