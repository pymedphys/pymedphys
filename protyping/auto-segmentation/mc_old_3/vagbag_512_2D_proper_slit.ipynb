{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "path = \"./vet_dataset_cleaned/\"\n",
    "test_perc = 0.15\n",
    "\n",
    "# The meaning of life\n",
    "random.seed(42)  \n",
    "np.random.seed(42)\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import paths as paths\n",
    "\n",
    "\n",
    "patient_paths = paths.get_patient_paths(path)\n",
    "patient_paths.sort()\n",
    "\n",
    "img_paths = [glob.glob(path + \"/img/*\") for path in patient_paths]\n",
    "mask_paths = [glob.glob(path + \"/mask/*\") for path in patient_paths]\n",
    "\n",
    "swap = int(len(img_paths) * test_perc)\n",
    "\n",
    "print(\"Total:\", swap + len(img_paths))\n",
    "print(\"Training patients:\", len(img_paths))\n",
    "print(\"Testing patients:\", swap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = paths.flatten_list(img_paths[:-swap])\n",
    "train_truths = paths.flatten_list(mask_paths[:-swap])\n",
    "\n",
    "train_inputs.sort()\n",
    "train_truths.sort()\n",
    "\n",
    "test_inputs = paths.flatten_list(img_paths[-swap:])\n",
    "test_truths = paths.flatten_list(mask_paths[-swap:])\n",
    "\n",
    "test_inputs.sort()\n",
    "train_truths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_together = list(zip(train_inputs, train_truths))\n",
    "random.shuffle(shuffle_together)\n",
    "train_inputs, train_truths = zip(*shuffle_together)\n",
    "\n",
    "train_inputs = np.array([np.load(array) for array in train_inputs])\n",
    "train_truths = np.array([np.load(array) for array in train_truths])\n",
    "\n",
    "print(\"Training inputs: \", train_inputs.shape)\n",
    "print(\"Training truths: \", train_truths.shape)\n",
    "\n",
    "\n",
    "data_mean = np.mean(train_inputs)\n",
    "data_std = np.std(train_inputs)\n",
    "print(data_mean)\n",
    "print(data_std)\n",
    "\n",
    "train_inputs = (train_inputs - data_mean) / data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# _ = plt.hist(train_inputs.flatten(), bins=200)\n",
    "# plt.xlim(-3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = paths.flatten_list(img_paths[-swap:])\n",
    "test_truths = paths.flatten_list(mask_paths[-swap:])\n",
    "\n",
    "test_inputs = np.array([np.load(array) for array in test_inputs])\n",
    "test_truths = np.array([np.load(array) for array in test_truths])\n",
    "\n",
    "\n",
    "print(\"Testing inputs: \", test_inputs.shape)\n",
    "print(\"Testing truths: \", test_truths.shape)\n",
    "\n",
    "test_inputs = (test_inputs - data_mean) / data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0 \n",
    "perc_vacbag = np.sum(train_truths[...,index] > 0) / len(train_truths[...,index].flatten())\n",
    "print(perc_vacbag)\n",
    "print(1-perc_vacbag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberGenerator(n):\n",
    "     number = 0\n",
    "     while number < n:\n",
    "         yield number\n",
    "         number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "\n",
    "import cv2\n",
    "import skimage.transform\n",
    "\n",
    "\n",
    "def gaussian_noise(img, mean=0, sigma=0.003):\n",
    "    img = img.copy()\n",
    "    noise = np.random.normal(mean, sigma, img.shape)\n",
    "    mask_overflow_upper = img+noise >= 1.0\n",
    "    mask_overflow_lower = img+noise < 0\n",
    "    noise[mask_overflow_upper] = 1.0\n",
    "    noise[mask_overflow_lower] = 0\n",
    "    img = img + noise\n",
    "    return img\n",
    "\n",
    "def random_crop_resize(img, label, crop_size=492):\n",
    "    size_img = img.shape\n",
    "    size_label = label.shape\n",
    "    crop_size = random.randint(crop_size, img.shape[0]-1)\n",
    "    crop_size = (crop_size, crop_size)\n",
    "\n",
    "    # \"Crop size should be less than image size\"\n",
    "    assert crop_size[0] <= img.shape[0] and crop_size[1] <= img.shape[1]\n",
    "\n",
    "    w, h = img.shape[:2]\n",
    "    x, y = np.random.randint(h-crop_size[0]), np.random.randint(w-crop_size[1])\n",
    "\n",
    "    img = img[y:y+crop_size[0], x:x+crop_size[1],:]\n",
    "    img = skimage.transform.resize(img, size_img)\n",
    "\n",
    "    label = label[y:y+crop_size[0], x:x+crop_size[1],:]\n",
    "    label = skimage.transform.resize(label, size_label)\n",
    "    return img, label\n",
    "\n",
    "def affine_transform(image, label, alpha_affine=0.5, random_state=None):\n",
    "\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape\n",
    "    shape_size = shape[:2]\n",
    "    center_square = np.float32(shape_size) // 2\n",
    "    square_size = min(shape_size) // 3\n",
    "    pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n",
    "    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "\n",
    "    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
    "    image = image[...,np.newaxis]\n",
    "    label = cv2.warpAffine(label, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def data_augment(img, mask, chance=1):\n",
    "    # flip l/r\n",
    "    if random.uniform(0,1) < chance:\n",
    "        img = cv2.flip( img, 1 )\n",
    "        mask = cv2.flip( mask, 1 )\n",
    "        if len(img.shape) == 2:\n",
    "            img = img[...,np.newaxis]\n",
    "        if len(mask.shape) == 2:\n",
    "            mask = mask[...,np.newaxis]\n",
    "\n",
    "    # random crop and resize\n",
    "    if random.uniform(0,1) < chance:\n",
    "        img, mask = random_crop_resize(img, mask)\n",
    "        if len(mask.shape) == 2:\n",
    "            label = label[...,np.newaxis]\n",
    "\n",
    "    # random affine transformation\n",
    "    if random.uniform(0,1) < chance:\n",
    "        img, mask = affine_transform(img, mask, alpha_affine=20)\n",
    "        if len(img.shape) == 2:\n",
    "            img = img[...,np.newaxis]\n",
    "        if len(mask.shape) == 2:\n",
    "            mask = mask[...,np.newaxis]\n",
    "            \n",
    "    # random gaussian noise\n",
    "    if random.uniform(0,1) < chance:\n",
    "        img = gaussian_noise(img)\n",
    "        \n",
    "    return img, mask\n",
    "\n",
    "\n",
    "def read_array_list(arr_path_list):\n",
    "    return np.array([np.load(arr_path) for arr_path in arr_path_list])\n",
    "\n",
    "\n",
    "def data_generator(input_paths, truth_paths, batch_size, augment=False):\n",
    "    batch_number = 0\n",
    "    while batch_number < len(input_paths) / batch_size:\n",
    "        batch_input_paths = input_paths[batch_number: batch_number + batch_size]\n",
    "        batch_truth_paths = truth_paths[batch_number: batch_number + batch_size]\n",
    "        batch_number += batch_size\n",
    "        \n",
    "        batch_imgs = []\n",
    "        batch_masks = []\n",
    "        \n",
    "        for x, y in zip(batch_input_paths, batch_truth_paths):\n",
    "            if augment is True:\n",
    "                x = np.load(x)\n",
    "                y = np.load(y)\n",
    "                if random.uniform(0,1) < 0.5:\n",
    "                    x, y = data_augment(x, y, chance=0.5)\n",
    "\n",
    "            batch_imgs.append(x)\n",
    "            batch_masks.append(y)\n",
    "        \n",
    "        yield np.array(batch_imgs), np.array(batch_masks)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = data_generator(train_inputs, train_truths, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y[0,...,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train as train\n",
    "import tensorflow as tf\n",
    "\n",
    "INITIAL_LR = 1e-4\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(lr = INITIAL_LR)\n",
    "LOSS = tf.keras.losses.BinaryCrossentropy()\n",
    "WEIGHTS = \"./vacbag/initial.hdf5\"\n",
    "\n",
    "\n",
    "model = train.compile_model(1, OPTIMIZER, LOSS, weights=WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import explore as explore\n",
    "explore.plot_batch_predict(model, test_inputs, test_truths, x=0, batch_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE = \"./vacbag/delete\"\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 8\n",
    "LR_SCALE = 0.5\n",
    "LR_PATIENCE = 3\n",
    "STOP_PATIENCE = 100\n",
    "\n",
    "model, train_history = train.train_model(model, train_inputs, train_truths, \n",
    "                                  MODEL_SAVE, \n",
    "                                  EPOCHS, \n",
    "                                  BATCH_SIZE, \n",
    "                                  LR_SCALE, \n",
    "                                  LR_PATIENCE, \n",
    "                                  STOP_PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_history.history)\n",
    "\n",
    "train.plot_metric(train_history, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.plot_loss(train_history)\n",
    "explore.plot_batch_predict(model, test_inputs, test_truths, x=0, batch_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss as loss\n",
    "\n",
    "INITIAL_LR = 1e-4\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(lr = INITIAL_LR)\n",
    "LOSS = loss.dsc_loss\n",
    "WEIGHTS = \"./vacbag/bce_10_0.02_2020-04-29-15-53-02.hdf5\"\n",
    "\n",
    "model = train.compile_model(1, OPTIMIZER, LOSS, weights=WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE = \"./vacbag/dice_after_bce\"\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 5\n",
    "LR_SCALE = 0.5\n",
    "LR_PATIENCE = 3\n",
    "STOP_PATIENCE = 100\n",
    "\n",
    "\n",
    "train_history = train.train_model(model, train_inputs, train_truths, \n",
    "                                  MODEL_SAVE, \n",
    "                                  EPOCHS, \n",
    "                                  BATCH_SIZE, \n",
    "                                  LR_SCALE, \n",
    "                                  LR_PATIENCE, \n",
    "                                  STOP_PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.plot_batch_predict(model, test_inputs, test_truths, x=0, batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_history = model.evaluate(x=test_inputs,\n",
    "                        y=test_truths,\n",
    "                        batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.plot_loss(eval_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss as loss\n",
    "import train as train\n",
    "\n",
    "INITIAL_LR = 1e-6\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(lr = INITIAL_LR)\n",
    "LOSS = loss.dsc_loss\n",
    "WEIGHTS = \"./vacbag/dice_after_bce_41_0.04_2020-04-29-16-21-27.hdf5\"\n",
    "\n",
    "model = train.compile_model(1, OPTIMIZER, LOSS, weights=WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_history = model.evaluate(x=test_inputs,\n",
    "                        y=test_truths,\n",
    "                        batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import explore as explore\n",
    "explore.plot_batch_predict(model, test_inputs, test_truths, x=5, batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_inputs[7,...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_truths[7,...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
