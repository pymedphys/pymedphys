{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes it so any changes in pymedphys is automatically\n",
    "# propagated into the notebook without needing a kernel reset.\n",
    "from IPython.lib.deepreload import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymedphys.labs.autosegmentation import (\n",
    "    indexing, softdice, filtering, pipeline, tfrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all of the DICOM data within a directory called 'dicom' in here:\n",
    "data_path_root = pathlib.Path.home().joinpath('.data/dicom-ct-and-structures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ct_image_paths, \n",
    "    structure_set_paths, \n",
    "    ct_uid_to_structure_uid, \n",
    "    structure_uid_to_ct_uids,\n",
    ") = indexing.get_uid_cache(data_path_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_map = filtering.load_names_mapping('name_mappings.json')\n",
    "\n",
    "# Used to verify that all structures have either been ignored or mapped to a name\n",
    "filtering.verify_all_names_have_mapping(data_path_root, structure_set_paths, names_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    structure_names_by_ct_uid, \n",
    "    structure_names_by_structure_set_uid,\n",
    ") = indexing.get_cached_structure_names_by_uids(\n",
    "    data_path_root, \n",
    "    structure_set_paths, \n",
    "    names_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list_of_structures = list(set(\n",
    "    [item for key, item in names_map.items()]\n",
    ").difference({None}))\n",
    "\n",
    "full_list_of_structures = sorted(full_list_of_structures)\n",
    "full_list_of_structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for the following structures, in the following order\n",
    "structures_to_learn = [\n",
    "    'lens_left', 'lens_right', 'eye_left', 'eye_right', 'patient']\n",
    "\n",
    "# Only use a study set if all of the following are defined on that study set\n",
    "study_set_must_have_all_of = structures_to_learn\n",
    "\n",
    "# Only use a slice if one of the following contours exists on it\n",
    "slice_at_least_one_of = [\n",
    "    'lens_left', 'lens_right', 'eye_left', 'eye_right']\n",
    "slice_must_have = ['patient']\n",
    "slice_cannot_have = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ct_uids = filtering.filter_ct_uids(\n",
    "    structure_uid_to_ct_uids,\n",
    "    structure_names_by_structure_set_uid,\n",
    "    structure_names_by_ct_uid,\n",
    "    study_set_must_have_all_of,\n",
    "    slice_at_least_one_of,\n",
    "    slice_must_have,\n",
    "    slice_cannot_have,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(filtered_ct_uids)\n",
    "\n",
    "dataset = pipeline.create_numpy_generator_dataset(\n",
    "    data_path_root,\n",
    "    structure_set_paths,\n",
    "    ct_image_paths,\n",
    "    ct_uid_to_structure_uid,\n",
    "    names_map,\n",
    "    filtered_ct_uids,\n",
    "    structures_to_learn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ct_uid, x_grid, y_grid, input_array, output_array in dataset.take(15):\n",
    "    print(ct_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_path = str(data_path_root.joinpath(\n",
    "    'lense-eye-patient.tfrecord'))\n",
    "tfrecord.write(tfrecord_path, dataset.take(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dataset = tfrecord.read(tfrecord_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ct_uid, x_grid, y_grid, input_array, output_array in loaded_dataset.take(15):\n",
    "    print(ct_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run timings to compare the numpy generator pulling from cache to the tfrecord dataset\n",
    "\n",
    "While the numpy generator is super flexible, able to be generated a fresh from a set of DICOM\n",
    "files and it will fall back to creating the mask if a cache doesn't exist it is about 50%\n",
    "slower than the tfrecord dataset.\n",
    "\n",
    "Nevertheless, both the tfrecord dataset and the number generator dataset take on the order of\n",
    "10 ms / item for this example with 5 masks.\n",
    "\n",
    "At that time cost, there doesn't seem to be a massive advantage for using the tfrecord.\n",
    "The trade off in loss of flexibility doesn't appear to be worth it.\n",
    "\n",
    "Nevertheless, there may be a significant time difference should the numpy generator need to\n",
    "copy the data onto the GPU. Potentially the tfrecord dataset may just load the data directly\n",
    "onto the GPU saving a copy step. Will need to determine experimentally which dataset provides\n",
    "the best development flexibility / training time trade off. However, I suspect the small\n",
    "time reduction that tfrecord provides won't offset the loss in flexibility from using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# Numpy generator dataset\n",
    "for ct_uid, x_grid, y_grid, input_array, output_array in dataset.take(15):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# tfrecord dataset\n",
    "for ct_uid, x_grid, y_grid, input_array, output_array in loaded_dataset.take(15):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymedphys-master",
   "language": "python",
   "name": "pymedphys-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
