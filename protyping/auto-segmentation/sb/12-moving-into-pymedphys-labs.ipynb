{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import pathlib\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shapely.geometry\n",
    "import skimage.draw\n",
    "import skimage.filters\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pydicom\n",
    "\n",
    "import pymedphys\n",
    "import pymedphys._dicom.structure as dcm_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('name_mappings.json') as f:\n",
    "    name_mappings_config = json.load(f)\n",
    "    names_map = name_mappings_config[\"names_map\"]\n",
    "    ignore_list = name_mappings_config[\"ignore_list\"]\n",
    "    \n",
    "    for key in ignore_list:\n",
    "        names_map[key] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to verify that all structures have either been ignored or mapped to a name\n",
    "\n",
    "# names = set()\n",
    "\n",
    "# for uid, path in structure_set_paths.items():\n",
    "#     dcm = pydicom.read_file(\n",
    "#         path, force=True, specific_tags=['StructureSetROISequence'])\n",
    "#     for item in dcm.StructureSetROISequence:\n",
    "#         names.add(item.ROIName)\n",
    "\n",
    "# mapped_names = set(names_map.keys())\n",
    "# print(mapped_names.difference(names))\n",
    "# names.difference(mapped_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_list_of_structures = list(set([item for key, item in names_map.items()]).difference({None}))\n",
    "# full_list_of_structures = sorted(full_list_of_structures)\n",
    "# full_list_of_structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for the following structures, in the following order\n",
    "structures_to_learn = [\n",
    "    'lens_left', 'lens_right', 'eye_left', 'eye_right', 'patient']\n",
    "\n",
    "# Only use a study set if all of the following are defined on that study set\n",
    "study_set_filter_must_have_all_of = set(structures_to_learn)\n",
    "\n",
    "# Only use a slice if one of the following contours exists on it\n",
    "slice_filter_at_least_one_of = set([\n",
    "    'lens_left', 'lens_right', 'eye_left', 'eye_right'])\n",
    "slice_filter_must_have = set(['patient'])\n",
    "slice_filter_cannot_have = set([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all of the SASH DICOM data within a directory called 'dicom' in here:\n",
    "data_path_root = pathlib.Path.home().joinpath('.data/dicom-ct-and-structures')\n",
    "dcm_paths = list(data_path_root.rglob('dicom/**/*.dcm'))\n",
    "\n",
    "# This will be the location of the numpy cache\n",
    "npz_directory = data_path_root.joinpath('npz_cache')\n",
    "npz_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# This will be the location of the tfrecord cache\n",
    "tfrecord_directory = data_path_root.joinpath('tfrecord_cache')\n",
    "tfrecord_directory.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the location of the DICOM header UID cache\n",
    "uid_cache_path = data_path_root.joinpath(\"uid-cache.json\")\n",
    "\n",
    "# This will be the location of structure names by UID cache\n",
    "structure_names_cache_path = data_path_root.joinpath(\"structure-names-cache.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_surface_dice(reference, evaluation):\n",
    "    edge_reference = skimage.filters.scharr(reference)\n",
    "    edge_evaluation = skimage.filters.scharr(evaluation)\n",
    "    \n",
    "    score = (\n",
    "        np.sum(np.abs(edge_evaluation - edge_reference)) /\n",
    "        np.sum(edge_evaluation + edge_reference)\n",
    "    )\n",
    "    \n",
    "    return 1 - score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uid_cache(relative_paths):\n",
    "    relative_paths = [\n",
    "        str(path) for path in relative_paths\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        with open(uid_cache_path) as f:\n",
    "            uid_cache = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        uid_cache = {\n",
    "            \"ct_image_paths\": {},\n",
    "            \"structure_set_paths\": {},\n",
    "            \"ct_uid_to_structure_uid\": {},\n",
    "            \"paths_when_run\": []\n",
    "        }\n",
    "    \n",
    "    if set(uid_cache[\"paths_when_run\"]) == set(relative_paths):\n",
    "        return uid_cache\n",
    "    \n",
    "    dcm_headers = []\n",
    "    for dcm_path in dcm_paths:\n",
    "        dcm_headers.append(pydicom.read_file(\n",
    "            dcm_path, force=True, \n",
    "            specific_tags=['SOPInstanceUID', 'SOPClassUID', 'StudyInstanceUID']))\n",
    "        \n",
    "    ct_image_paths = {\n",
    "        str(header.SOPInstanceUID): str(path)\n",
    "        for header, path in zip(dcm_headers, relative_paths)\n",
    "        if header.SOPClassUID.name == \"CT Image Storage\"\n",
    "    }\n",
    "    \n",
    "    structure_set_paths = {\n",
    "        str(header.SOPInstanceUID): str(path)\n",
    "        for header, path in zip(dcm_headers, relative_paths)\n",
    "        if header.SOPClassUID.name == \"RT Structure Set Storage\"\n",
    "    }\n",
    "    \n",
    "    ct_uid_to_study_instance_uid = {\n",
    "        str(header.SOPInstanceUID): str(header.StudyInstanceUID)\n",
    "        for header in dcm_headers\n",
    "        if header.SOPClassUID.name == \"CT Image Storage\"\n",
    "    }\n",
    "    \n",
    "    study_instance_uid_to_structure_uid = {\n",
    "        str(header.StudyInstanceUID): str(header.SOPInstanceUID)\n",
    "        for header in dcm_headers\n",
    "        if header.SOPClassUID.name == \"RT Structure Set Storage\"\n",
    "    }\n",
    "    \n",
    "    ct_uid_to_structure_uid = {\n",
    "        ct_uid: study_instance_uid_to_structure_uid[study_uid]\n",
    "        for ct_uid, study_uid in ct_uid_to_study_instance_uid.items()\n",
    "    }\n",
    "    \n",
    "    uid_cache[\"ct_image_paths\"] = ct_image_paths\n",
    "    uid_cache[\"structure_set_paths\"] = structure_set_paths\n",
    "    uid_cache[\"ct_uid_to_structure_uid\"] = ct_uid_to_structure_uid    \n",
    "    uid_cache[\"paths_when_run\"] = relative_paths\n",
    "    \n",
    "    with open(uid_cache_path, \"w\") as f:\n",
    "        json.dump(uid_cache, f)\n",
    "        \n",
    "    return uid_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_paths = [\n",
    "    path.relative_to(data_path_root)\n",
    "    for path in dcm_paths\n",
    "]\n",
    "\n",
    "uid_cache = get_uid_cache(relative_paths)\n",
    "ct_image_paths = uid_cache[\"ct_image_paths\"]\n",
    "structure_set_paths = uid_cache[\"structure_set_paths\"]\n",
    "ct_uid_to_structure_uid = uid_cache[\"ct_uid_to_structure_uid\"]\n",
    "\n",
    "structure_uid_to_ct_uids = {}\n",
    "for ct_uid, structure_uid in ct_uid_to_structure_uid.items():\n",
    "    try:\n",
    "        structure_uid_to_ct_uids[structure_uid].append(ct_uid)\n",
    "    except KeyError:\n",
    "        structure_uid_to_ct_uids[structure_uid] = [ct_uid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_transformation_parameters(dcm_ct):\n",
    "    # From Matthew Coopers work in ../old/data_generator.py\n",
    "    \n",
    "    position = dcm_ct.ImagePositionPatient\n",
    "    spacing = [x for x in dcm_ct.PixelSpacing] + [dcm_ct.SliceThickness]\n",
    "    orientation = dcm_ct.ImageOrientationPatient\n",
    "\n",
    "    dx, dy, *_ = spacing\n",
    "    Cx, Cy, *_ = position\n",
    "    Ox, Oy = orientation[0], orientation[4]\n",
    "    \n",
    "    return dx, dy, Cx, Cy, Ox, Oy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_expanded_mask(expanded_mask, img_size, expansion):\n",
    "    return np.mean(np.mean(\n",
    "        tf.reshape(expanded_mask, (img_size, expansion, img_size, expansion)),\n",
    "        axis=1), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aliased_mask(contours, dcm_ct, expansion=5):\n",
    "    dx, dy, Cx, Cy, Ox, Oy = get_image_transformation_parameters(dcm_ct)\n",
    "    \n",
    "    ct_size = np.shape(dcm_ct.pixel_array)\n",
    "    x_grid = np.arange(Cx, Cx + ct_size[0]*dx*Ox, dx*Ox)\n",
    "    y_grid = np.arange(Cy, Cy + ct_size[1]*dy*Oy, dy*Oy)\n",
    "    \n",
    "    new_ct_size = np.array(ct_size) * expansion\n",
    "    \n",
    "    expanded_mask = np.zeros(new_ct_size)\n",
    "    \n",
    "    for xyz in contours:\n",
    "        x = np.array(xyz[0::3])\n",
    "        y = np.array(xyz[1::3])\n",
    "        z = xyz[2::3]\n",
    "\n",
    "        assert len(set(z)) == 1\n",
    "\n",
    "        r = (((y - Cy) / dy * Oy)) * expansion + (expansion - 1) * 0.5\n",
    "        c = (((x - Cx) / dx * Ox)) * expansion + (expansion - 1) * 0.5\n",
    "\n",
    "        expanded_mask = np.logical_or(\n",
    "            expanded_mask, \n",
    "            skimage.draw.polygon2mask(new_ct_size, np.array(list(zip(r, c)))))\n",
    "        \n",
    "    mask = reduce_expanded_mask(expanded_mask, ct_size[0], expansion)\n",
    "    mask = 2 * mask - 1\n",
    "    \n",
    "    return x_grid, y_grid, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contours_from_mask(x_grid, y_grid, mask):\n",
    "    cs = plt.contour(x_grid, y_grid, mask, [0]);\n",
    "    \n",
    "    contours = [\n",
    "        path.vertices for path in cs.collections[0].get_paths()\n",
    "    ]\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structure_names_by_uids(structure_set_paths, names_map):\n",
    "    structure_names_by_ct_uid = {}\n",
    "    structure_names_by_structure_set_uid = {}\n",
    "\n",
    "\n",
    "    for structure_set_uid, relative_structure_set_path in structure_set_paths.items():\n",
    "        structure_set_path = data_path_root.joinpath(relative_structure_set_path)\n",
    "\n",
    "        structure_set = pydicom.read_file(\n",
    "            structure_set_path, \n",
    "            force=True, \n",
    "            specific_tags=['ROIContourSequence', 'StructureSetROISequence'])\n",
    "\n",
    "        number_to_name_map = {\n",
    "            roi_sequence_item.ROINumber: names_map[roi_sequence_item.ROIName]\n",
    "            for roi_sequence_item in structure_set.StructureSetROISequence\n",
    "            if names_map[roi_sequence_item.ROIName] is not None\n",
    "        }\n",
    "\n",
    "        structure_names_by_structure_set_uid[structure_set_uid] = [\n",
    "            item for _, item in number_to_name_map.items()]\n",
    "\n",
    "\n",
    "        for roi_contour_sequence_item in structure_set.ROIContourSequence:\n",
    "            try:\n",
    "                structure_name = number_to_name_map[roi_contour_sequence_item.ReferencedROINumber]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            for contour_sequence_item in roi_contour_sequence_item.ContourSequence:\n",
    "                ct_uid = contour_sequence_item.ContourImageSequence[0].ReferencedSOPInstanceUID\n",
    "\n",
    "                try:\n",
    "                    structure_names_by_ct_uid[ct_uid].add(structure_name)\n",
    "                except KeyError:\n",
    "                    structure_names_by_ct_uid[ct_uid] = set([structure_name])\n",
    "    \n",
    "    structure_names_by_ct_uid = {\n",
    "        key: list(item) for key, item in structure_names_by_ct_uid.items()\n",
    "    }\n",
    "    \n",
    "    return structure_names_by_ct_uid, structure_names_by_structure_set_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_structure_names_by_uids(structure_set_paths, names_map):\n",
    "    structure_set_paths = {\n",
    "        str(key): str(item) for key, item in structure_set_paths.items()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(structure_names_cache_path) as f:\n",
    "            structure_names_cache = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        structure_names_cache = {\n",
    "            \"structure_names_by_ct_uid\": {},\n",
    "            \"structure_names_by_structure_set_uid\": {},\n",
    "            \"structure_set_paths_when_run\": {},\n",
    "            \"names_map_when_run\": {}\n",
    "        }\n",
    "        \n",
    "    cache_valid = (\n",
    "        structure_names_cache[\"structure_set_paths_when_run\"] == structure_set_paths and\n",
    "        structure_names_cache[\"names_map_when_run\"] == names_map)\n",
    "    \n",
    "    if cache_valid:\n",
    "        return structure_names_cache\n",
    "    \n",
    "    structure_names_by_ct_uid, structure_names_by_structure_set_uid = (\n",
    "        get_structure_names_by_uids(structure_set_paths, names_map))\n",
    "    \n",
    "    structure_names_cache[\"structure_names_by_ct_uid\"] = structure_names_by_ct_uid\n",
    "    structure_names_cache[\n",
    "        \"structure_names_by_structure_set_uid\"] = structure_names_by_structure_set_uid\n",
    "    structure_names_cache[\"structure_set_paths_when_run\"] = structure_set_paths\n",
    "    structure_names_cache[\"names_map_when_run\"] = names_map\n",
    "            \n",
    "    with open(structure_names_cache_path, \"w\") as f:\n",
    "        json.dump(structure_names_cache, f)\n",
    "    \n",
    "    return structure_names_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_names_cache = get_cached_structure_names_by_uids(\n",
    "    structure_set_paths, names_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_names_by_ct_uid = structure_names_cache[\"structure_names_by_ct_uid\"]\n",
    "structure_names_by_structure_set_uid = structure_names_cache[\n",
    "    \"structure_names_by_structure_set_uid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure_names_by_ct_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure_names_by_structure_set_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_uids_to_train_on = []\n",
    "\n",
    "for structure_uid, ct_uids in structure_uid_to_ct_uids.items():\n",
    "    structure_names_in_study_set = set(structure_names_by_structure_set_uid[structure_uid])\n",
    "    \n",
    "    if not structure_names_in_study_set.issuperset(\n",
    "        study_set_filter_must_have_all_of\n",
    "    ):\n",
    "        continue\n",
    "        \n",
    "    for ct_uid in ct_uids:\n",
    "        try:\n",
    "            structure_names_on_slice = set(structure_names_by_ct_uid[ct_uid])\n",
    "        except KeyError:\n",
    "            structure_names_on_slice = set([])\n",
    "            \n",
    "        if len(structure_names_on_slice.intersection(slice_filter_at_least_one_of)) == 0:\n",
    "            continue\n",
    "            \n",
    "        if not structure_names_on_slice.issuperset(slice_filter_must_have):\n",
    "            continue\n",
    "            \n",
    "        if len(structure_names_on_slice.intersection(slice_filter_cannot_have)) != 0:\n",
    "            continue\n",
    "            \n",
    "        ct_uids_to_train_on.append(ct_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct_uids_to_train_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len({1, 2,3, 4}.intersection({2,3,5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numpy_input_output(ct_uid):\n",
    "    structure_uid = ct_uid_to_structure_uid[ct_uid]\n",
    "\n",
    "    structure_set_path = data_path_root.joinpath(structure_set_paths[structure_uid])\n",
    "\n",
    "    structure_set = pydicom.read_file(\n",
    "        structure_set_path, \n",
    "        force=True, \n",
    "        specific_tags=['ROIContourSequence', 'StructureSetROISequence'])\n",
    "    \n",
    "    number_to_name_map = {\n",
    "        roi_sequence_item.ROINumber: names_map[roi_sequence_item.ROIName]\n",
    "        for roi_sequence_item in structure_set.StructureSetROISequence\n",
    "        if names_map[roi_sequence_item.ROIName] is not None\n",
    "    }\n",
    "    \n",
    "    contours_by_ct_uid = {}\n",
    "\n",
    "    for roi_contour_sequence_item in structure_set.ROIContourSequence:\n",
    "        try:\n",
    "            structure_name = number_to_name_map[roi_contour_sequence_item.ReferencedROINumber]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        for contour_sequence_item in roi_contour_sequence_item.ContourSequence:\n",
    "            ct_uid = contour_sequence_item.ContourImageSequence[0].ReferencedSOPInstanceUID\n",
    "\n",
    "            try:\n",
    "                _ = contours_by_ct_uid[ct_uid]\n",
    "            except KeyError:\n",
    "                contours_by_ct_uid[ct_uid] = dict()\n",
    "\n",
    "            try:\n",
    "                contours_by_ct_uid[ct_uid][structure_name].append(contour_sequence_item.ContourData)\n",
    "            except KeyError:\n",
    "                contours_by_ct_uid[ct_uid][structure_name] = [contour_sequence_item.ContourData]\n",
    "                \n",
    "    ct_path = data_path_root.joinpath(ct_image_paths[ct_uid])\n",
    "    dcm_ct = pydicom.read_file(ct_path, force=True)\n",
    "    dcm_ct.file_meta.TransferSyntaxUID = pydicom.uid.ImplicitVRLittleEndian\n",
    "\n",
    "    ct_size = np.shape(dcm_ct.pixel_array)\n",
    "    \n",
    "    contours_on_this_slice = contours_by_ct_uid[ct_uid].keys()\n",
    "\n",
    "    masks = np.nan * np.ones((*ct_size, len(structures_to_learn)))\n",
    "\n",
    "    for i, structure in enumerate(structures_to_learn):\n",
    "        if not structure in contours_on_this_slice:\n",
    "            masks[:,:,i] = np.zeros(ct_size) - 1\n",
    "\n",
    "            continue\n",
    "\n",
    "        original_contours = contours_by_ct_uid[ct_uid][structure]\n",
    "        x_grid, y_grid, masks[:,:,i] = calculate_aliased_mask(original_contours, dcm_ct)\n",
    "        \n",
    "    np.shape(masks)\n",
    "    assert np.sum(np.isnan(masks)) == 0\n",
    "    \n",
    "    return dcm_ct.pixel_array, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_input_output_from_cache(ct_uid, structures_to_learn):\n",
    "    npz_path = npz_directory.joinpath(f'{ct_uid}.npz')\n",
    "    structures_to_learn_cache_path = npz_directory.joinpath('structures_to_learn.json')\n",
    "    \n",
    "    try:\n",
    "        with open(structures_to_learn_cache_path) as f:\n",
    "            structures_to_learn_cache = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        structures_to_learn_cache = []\n",
    "        \n",
    "    if structures_to_learn_cache != structures_to_learn:\n",
    "        logging.warning(\"Structures to learn has changed. Dumping npz cache.\")\n",
    "        for path in npz_directory.glob('*.npz'):\n",
    "            path.unlink()\n",
    "        with open(structures_to_learn_cache_path, \"w\") as f:\n",
    "            json.dump(structures_to_learn, f)\n",
    "\n",
    "    try:\n",
    "        data = np.load(npz_path)\n",
    "        input_array = data['input_array']\n",
    "        output_array = data['output_array']\n",
    "    except FileNotFoundError:\n",
    "        input_array, output_array = create_numpy_input_output(ct_uid)\n",
    "        np.savez(npz_path, input_array=input_array, output_array=output_array)\n",
    "        \n",
    "    return input_array, output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ct_uids = list(ct_image_paths.keys())\n",
    "# random.shuffle(all_ct_uids)\n",
    "\n",
    "random.shuffle(ct_uids_to_train_on)\n",
    "\n",
    "def from_numpy_generator():\n",
    "    for ct_uid in ct_uids_to_train_on:\n",
    "        input_array, output_array = numpy_input_output_from_cache(\n",
    "            ct_uid, structures_to_learn)\n",
    "        input_array = input_array[:,:,None]\n",
    "        \n",
    "        yield ct_uid, input_array, output_array\n",
    "        \n",
    "from_numpy_generator_params = (\n",
    "    (tf.string, tf.int32, tf.float64),\n",
    "    (\n",
    "        tf.TensorShape(()), \n",
    "        tf.TensorShape([512, 512, 1]), \n",
    "        tf.TensorShape([512, 512, len(structures_to_learn)]))\n",
    ")\n",
    "\n",
    "from_numpy_dataset = tf.data.Dataset.from_generator(\n",
    "    from_numpy_generator, *from_numpy_generator_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def serialise(ct_uid, input_array, output_array):\n",
    "    ct_uid = tf.io.serialize_tensor(ct_uid)\n",
    "    input_array = tf.io.serialize_tensor(input_array)\n",
    "    output_array = tf.io.serialize_tensor(output_array)\n",
    "    \n",
    "    feature = {\n",
    "        'ct_uid': _bytes_feature(ct_uid),\n",
    "        'input_array': _bytes_feature(input_array),\n",
    "        'output_array': _bytes_feature(output_array),\n",
    "    }\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "\n",
    "for ct_uid, input_array, output_array in from_numpy_dataset.take(1):\n",
    "    serialise(ct_uid, input_array, output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Details on this from https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "# def tf_serialise(ct_uid, input_array, output_array):\n",
    "#     tf_string = tf.py_function(\n",
    "#         serialise,\n",
    "#         (ct_uid, input_array, output_array),\n",
    "#         tf.string\n",
    "#     )\n",
    "#     return tf.reshape(tf_string, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialised_dataset = from_numpy_dataset.map(tf_serialise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialised_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_path = str(tfrecord_directory.joinpath(\n",
    "    'lense-eye-patient.tfrecord'))\n",
    "# writer = tf.data.experimental.TFRecordWriter(tfrecord_path)\n",
    "# writer.write(serialised_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "for ct_uid, input_array, output_array in from_numpy_dataset.take(100):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_features = {\n",
    "    'ct_uid': tf.io.FixedLenFeature([], tf.string),\n",
    "    'input_array': tf.io.FixedLenFeature([], tf.string),\n",
    "    'output_array': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_dataset(example_proto):\n",
    "    parsed = tf.io.parse_single_example(example_proto, parse_features)\n",
    "    ct_uid = tf.io.parse_tensor(parsed['ct_uid'], tf.string)\n",
    "    input_array = tf.io.parse_tensor(parsed['input_array'], tf.int32)\n",
    "    output_array = tf.io.parse_tensor(parsed['output_array'], tf.float64)\n",
    "\n",
    "    return ct_uid, input_array, output_array\n",
    "    \n",
    "\n",
    "parsed_dataset = raw_dataset.map(_parse_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "for ct_uid, input_array, output_array in parsed_dataset.take(100):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymedphys-master",
   "language": "python",
   "name": "pymedphys-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
